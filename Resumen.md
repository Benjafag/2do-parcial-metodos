# Unidad 3 - Sistemas de ecuaciones lineales

## üîç Objetivo

Resolver sistemas del tipo:

$$
A \cdot x = b
$$

donde $A \in \mathbb{R}^{n \times n}$, $x$ es el vector inc√≥gnita y $b$ el vector de constantes. Se usan m√©todos num√©ricos para evitar errores acumulativos y reducir el costo computacional.

## üî∏ Tipos de Matrices

**DENSAS:** Son aquellas que poseen pocos elementos nulos y son de orden bajo\
**RALAS (SPARSE):** Son aquellas que poseen muchos ceros y son de orden alto

## üî∏ Tipos de M√©todos

### 1. **M√©todos Directos**

Son aquellos que nos conducir√≠an a la soluci√≥n exacta luego de un n√∫mero finito de operaciones elementales, si no hubiera errores de redondeo

### üü¢ **Eliminaci√≥n de Gauss**

#### üîç Objetivo

Transforma el sistema en forma escalonada superior usando operaciones elementales a la matriz ampliada (modifica b) y luego se resuelve con **sustituci√≥n hacia atr√°s**.

**Errores comunes:**

- El error de redondeo se acumula en cada paso.
- Matrices mal condicionadas pueden amplificar errores.
- Sin pivoteo, puede fallar o generar grandes errores num√©ricos si hay ceros o valores muy peque√±os en la diagonal.


### üí° Ventajas

- M√©todo **general**: se puede aplicar a cualquier sistema no singular.
- Base de otros m√©todos como LU, Thomas (caso tridiagonal), y muchos algoritmos num√©ricos.
- Se puede mejorar con:

  - **Pivoteo parcial** (mayor estabilidad).
  - **Escalamiento** (evita errores por magnitudes muy diferentes).

### ‚ö†Ô∏è Requisitos

- La matriz debe ser **no singular**.

### üíª Complejidad

- Eliminaci√≥n (reducci√≥n a triangular superior):$ O\left( \frac{2}{3} n^3 \right)$
- Sustituci√≥n hacia atr√°s:$ O(n^2) $
- **Total:** $O(n^3)$

---

### üü¢ **Factorizaci√≥n LU**

#### üîç Objetivo

Descompone $A = L \cdot U$, donde $A$ debe ser **no singular**, y para LU sin pivoteo, debe ser posible evitar ceros en la diagonal de $U$:

- $L$: matriz triangular inferior con unos en la diagonal
- $U$: matriz triangular superior
- Luego resuelve $L \cdot y = b$, y $U \cdot x = y$

#### üí° Ventajas

- √ötil para resolver muchos sistemas con la misma matriz $A$ y distintos vectores $b$.
- Se puede calcular $|A| = |L.U| = |U|$
- C√°lculo de $A^{-1}$ usando como b las columnas can√≥nicas
- Se puede combinar con **pivoteo parcial escalado** para mayor estabilidad

### üíª Complejidad

- **Factorizaci√≥n LU:** $ O\left( \frac{2}{3} n^3 \right) $
- **Sustituciones (hacia adelante y hacia atr√°s):** $ O(n^2) $
- **Total por un sistema:** $ O(n^3) $
- **Total si ya ten√©s la factorizaci√≥n (solo resolver para otro $b$):** $ O(n^2) $

### **Pivoteo parcial**

- Wilkinson propuso que; en cada paso de la eliminaci√≥n de Gauss, buscar en la columna actual **entre las filas restantes** (desde la fila pivote hacia abajo)  el elemento con mayor valor absoluto y elegirlo como pivot, con el objetivo de evitar divisiones por n√∫meros muy peque√±os (o $0$) de tal forma que los multiplicadores $|m_{ik}| \leq 1$, evitando aumentar errores de redondeo.
- **Vector $P$ (permutaciones):**
  Registra el orden actual de las filas luego de los intercambios durante la eliminaci√≥n. Inicialmente, $P = [1, 2, ..., n]^t$. Cada vez que se intercambian filas $i$ y $k$, se intercambian tambi√©n $P_i \leftrightarrow P_k$. √ötil para mantener la correspondencia entre filas originales y filas actuales, sin mover f√≠sicamente toda la matriz (optimizaci√≥n).

### **Escalamiento Implicito**

- Wilkinson propone que una matriz debe equilibrarse antes de aplicar una algoritmo de solucion. Se basa en **normalizar la comparaci√≥n de valores en cada fila seg√∫n la magnitud relativa en su fila** para elegir mejor el pivote. Por lo que para cada fila $i$, se calcula su factor de escala en $S$:

- **Vector $S$ (escalas):**
  Guarda para cada fila $i$ el valor m√°ximo absoluto de sus coeficientes en la matriz original $A$:

$$
S_i = \max_{1 \leq j \leq n} |a_{ij}|
$$

- Al elegir el pivote, no se mira solo el valor absoluto del elemento de la columna $k$, sino el cociente: $ c_k =\max{1 \leq j \leq n} \frac{|a_{ij}^k| }{S_i} $ haciendo una comparaci√≥n justa y relativa

### ‚öôÔ∏è Funcionamiento resumido del ppe:

1. Se calcula $S$ antes de comenzar la eliminaci√≥n y $P$ se inicializa desde 1 hasya n.
2. En cada paso $k$, para elegir fila pivote $p$, se eval√∫a: $ c_i = \frac{|a_{ik}|}{S_i}$

3. Se intercambian filas $k$ y $p$ tanto en la matriz como en $P$.
4. Se contin√∫a con la eliminaci√≥n habitual.
5. Al resolver queda: $PLU x = b \implies L y = P b \implies U x = y$. Es decir, debemos aplicar la permutacion a b.

#### üìù Beneficios

- Permite hacer pivoteo con una comparaci√≥n justa considerando la escala de cada fila.
- Mejora estabilidad y evita errores num√©ricos.
- El vector $P$ facilita reconstruir la soluci√≥n o aplicar permutaciones posteriores sin perder datos.

---

### üü¢ Metodo de Gauss Jordan

Es similar al m√©todo de Gauss, la diferencia es que se diagonaliza la matriz

Al finalizar el algoritmo tenemos $ùë• = b^n$

Desventaja: Costo aumenta en 50%

### üü¢ Descomposici√≥n de Cholesky

#### üîç Objetivo

Resolver $A x = b$ cuando $A$ es **sim√©trica y definida positiva**.

#### üìê Descomposici√≥n:

$$
A = L \cdot L^T
$$

- $L$: matriz triangular inferior con coeficientes reales, incluso en su diagonal.
- Luego se resuelven:

$$
L y = b \quad \text{(sustituci√≥n hacia adelante)}
$$

$$
L^T x = y \quad \text{(sustituci√≥n hacia atr√°s)}
$$

### üßÆ F√≥rmulas por componentes

Para construir $L$, se recorren filas y columnas de forma incremental. Los elementos se calculan as√≠:

#### üü© Diagonal ($i = j$):

$$
\ell_{ii} = \sqrt{ a_{ii} - \sum_{k=1}^{i-1} \ell_{ik}^2 }
$$

#### üü¶ Debajo de la diagonal ($i > j$):

$$
\ell_{ji} = \frac{1}{\ell_{ii}} \left( a_{ji} - \sum_{k=1}^{i-1} \ell_{jk} \ell_{ik} \right)
$$

### üí° Ventajas

- Es **m√°s eficiente** y **m√°s estable** que la LU tradicional si se cumplen las condiciones.
- Requiere casi la **mitad del trabajo** que LU.
- No necesita pivoteo
- Ideal para matrices sim√©tricas dispersas.

### üíª Complejidad

- Descomposici√≥n: $O\left( \frac{1}{3} n^3 \right)$ (mitad que LU)
- Sustituciones: $O(n^2)$

---

### üü¢ M√©todo de Thomas (tridiagonal)

#### üîç Objetivo

Resolver sistemas donde $A$ es **tridiagonal**, s√≥lo tiene elementos distintos de cero en la diagonal principal y las diagonales adyacentes, es decir:

$$
\begin{bmatrix}f_1 & g_1 & 0   & \cdots & 0 \\e_2 & f_2 & g_2 & \cdots & 0 \\0   & e_3 & f_3 & \ddots & \vdots \\\vdots & \ddots & \ddots & \ddots & g_{n-1} \\0 & \cdots & 0 & e_n & f_n\end{bmatrix} .
\begin{bmatrix}x_1 \\x_2 \\x_3 \\\vdots \\x_{n-1} \\x_n\end{bmatrix} =
\begin{bmatrix}r_1 \\r_2 \\r_3 \\\vdots \\r_{n-1} \\r_n\end{bmatrix}
$$

### üß† Algoritmo

#### Descomposici√≥n

Para $k = 2$ hasta $n$: $e_k = \frac{e_k}{f_{k-1}}$; $f_k = f_k - e_k \cdot g_{k-1} $

#### Sustituci√≥n hacia adelante

Para $k = 2$ hasta $n$: $r_k = r_k - e_k \cdot r_{k-1} $

#### Sustituci√≥n hacia atr√°s

$x_n = {r_n}/{f_n}$ \
Para $k = n-1$ hasta $1, -1$ : $x_k = \frac{r_k - g_k \cdot x_{k+1}}{f_k} $

Es una forma optimizada de **eliminaci√≥n de Gauss** que aprovecha la estructura tridiagonal para:

1. **Eliminar los elementos debajo de la diagonal** de forma eficiente
2. **Resolver con sustituci√≥n hacia atr√°s**

### üí° Ventajas

- No requiere almacenar toda la matriz ‚Üí solo 3 vectores
- Precisi√≥n superior a m√©todos gen√©ricos al reducir operaciones

### üíª Complejidad

- Fase de eliminaci√≥n: $O(n)$
- Fase de sustituci√≥n: $O(n)$
- **Total:** $O(n)$

---

### üìå Comparativa final de m√©todos directos

| M√©todo       | Requisitos principales                      | Complejidad                                    | Ventajas clave                                                           |
| ------------ | ------------------------------------------- | ---------------------------------------------- | ------------------------------------------------------------------------ |
| **Gauss**    | No singular y $a_{ii} \neq 0$ (sin pivoteo) | $O(n^3)$                                       | General, robusto, base de muchos otros m√©todos                           |
| **LU**       | No singular y $a_{ii} \neq 0$ (sin pivoteo) | $O(n^3)$ (una vez) <br> $O(n^2)$ (por sistema) | Reutilizable para varios $b$, m√°s eficiente que Gauss en ese caso        |
| **Cholesky** | **Sim√©trica definida positiva**             | $O\left(\frac{1}{3} n^3 \right)$               | M√°s r√°pido y estable que LU si aplica; usa menos operaciones             |
| **Thomas**   | **Tridiagonal**                             | $O(n)$                                         | Extremadamente eficiente; ideal para sistemas grandes con esa estructura |

---

### 2. Metodos iterativos

## üîç Objetivo

Resolver sistemas lineales $A x = b$ mediante **aproximaciones sucesivas**, comenzando con una estimaci√≥n inicial $x^{(0)}$, que en principio convergen a la solucion x:

$$
x^{(k+1)} = B(x^{(k)}) + C
$$

B se llama matriz de iteraci√≥n, es una generalizaci√≥n del m√©todo de punto fijo.

Se aplican principalmente cuando el sistema es de **gran tama√±o** o **disperso (sparse)**.

### üí° Ventajas respecto a los metodos directos

- N√∫mero de operaciones
- Posiciones de memoria
- Errores de redondeo

## üî∏ Tipos de M√©todos Iterativos

- **Jacobi**
- **Gauss-Seidel**
- **SOR (Successive Over-Relaxation)**

---

## üü¢ M√©todo de Jacobi

### üîß Ecuaciones base

Dado el sistema:

$$
A x = b \quad \Rightarrow \quad a_{ii} x_i^{(k+1)} = b_i - \sum_{j \neq i} a_{ij} x_j^{(k)}
$$

$$
x_i^{(k+1)} = \frac{1}{a_{ii}} \left( b_i - \sum_{j \neq i} a_{ij} x_j^{(k)} \right)
$$

> Cada componente se calcula **usando solo valores de la iteraci√≥n anterior** $\rightarrow$ Aproximaciones simultaneas.

### üí° Ventajas

- Muy sencillo de implementar.
- F√°cil de paralelizar (cada ecuaci√≥n es independiente de las dem√°s en cada iteraci√≥n).

### ‚ö†Ô∏è Requisitos

- La matriz no debe tener ceros en la diagonal.

### üíª Complejidad

- Cada iteraci√≥n: $O(n^2)$
- N√∫mero de iteraciones depende de la convergencia (condici√≥n del sistema).

---

## üü¢ M√©todo de Gauss-Seidel

### üîß Ecuaci√≥n de iteraci√≥n

$$
x_i^{(k+1)} = \frac{1}{a_{ii}} \left( b_i - \sum_{j < i} a_{ij} x_j^{(k+1)} - \sum_{j > i} a_{ij} x_j^{(k)} \right)
$$

> Se usa **inmediatamente** cada nuevo valor calculado $x_j^{(k+1)}$ para actualizar el siguiente.

### üí° Ventajas

- Mejora notablemente la velocidad de convergencia respecto a Jacobi.
- Menor consumo de memoria (no requiere vector auxiliar).
- Adecuado para sistemas **grandes y dispersos**.

### ‚ö†Ô∏è Requisitos

- No siempre converge si A tiene mala condici√≥n.

### üíª Complejidad

- Cada iteraci√≥n: $O(n^2)$
- Menos iteraciones que Jacobi (aunque m√°s dif√≠ciles de paralelizar).

### üìâ Observaciones (Chapra)

- En la pr√°ctica, se usa como base para aceleraci√≥n (como en SOR).
- M√°s susceptible a errores si hay malas estimaciones iniciales.

---

## üü¢ M√©todo SOR (Relajaci√≥n Sucesiva)

Este m√©todo usa un factor de ponderaci√≥n para mejorar el valor calculado.

### üîß Ecuaci√≥n iterativa

$$
x_i^{(k+1)} = (1 - \omega) x_i^{(k)} + \frac{\omega}{a_{ii}} \left( b_i - \sum_{j < i} a_{ij} x_j^{(k+1)} - \sum_{j > i} a_{ij} x_j^{(k)} \right)
$$

- $\omega \in (0, 2)$: par√°metro de relajaci√≥n

  - $\omega = 1$ ‚Üí Gauss-Seidel
  - $\omega < 1$ ‚Üí subrelajaci√≥n
  - $\omega > 1$ ‚Üí **sobre-relajaci√≥n** (acelera convergencia si est√° bien elegido)

### üí° Ventajas

- Puede acelerar la convergencia de Gauss-Seidel de forma significativa.
- Muy √∫til en sistemas grandes y bien condicionados.

### ‚ö†Ô∏è Requisitos

- Requiere prueba emp√≠rica o an√°lisis para encontrar el $\omega$ √≥ptimo.

### üíª Complejidad

- Cada iteraci√≥n: $O(n^2)$
- Menos iteraciones que Gauss-Seidel si $\omega$ es adecuado

### üìâ Observaciones (Chapra)

- En muchos problemas pr√°cticos, un $\omega \in [1.1, 1.5]$ acelera fuertemente la convergencia.
- Chapra sugiere experimentar con distintos valores y observar el n√∫mero de iteraciones requeridas.

## üìå Comparativa final de m√©todos iterativos

| M√©todo           | Requisitos para convergencia                      | Convergencia | Paralelizaci√≥n | Velocidad relativa | Notas clave                          |
| ---------------- | ------------------------------------------------- | ------------ | -------------- | ------------------ | ------------------------------------ |
| **Jacobi**       | Diagonal dominante                                | Lenta        | Muy f√°cil      | üü° Lenta           | Simple, pero puede diverger          |
| **Gauss-Seidel** | Diagonal dominante, sim√©trica y definida positiva | R√°pida       | Dif√≠cil        | üü¢ Media           | Usa valores actualizados al instante |
| **SOR**          | Idem + $\omega$ adecuado                          | Muy r√°pida   | Dif√≠cil        | üü¢üü¢ R√°pida        | Acelera G-S si $\omega$ bien elegido |

---

## N√∫mero de condici√≥n

$K(A) = ||A|| \cdot ||A^{-1}||$

- Es un medida cuantitativa del grado de mal condicionamiento de la matriz de coeficientes. Mide cuan cerca est√° una matriz de ser singular
- Se usa para calcular como afectan los errores relativos en A y/o b el c√°lculo de x.
- Si A y b tienen t cifras significativas y Œ∫(A) es de un orden 10 s entonces la precisi√≥n del resultado ser√° 10 s-t
- Se puede demostrar que:

  $$\frac {||\delta x||} {||x||} \le K(A) \frac {||\delta b||}{||b||} $$
  $$\frac {||\delta x||} {||x + \delta x||} \le K(A) \frac {||\delta A||}{||A||} $$

Wilkinson estudi√≥ el efecto del redondeo en el m√©todo de Eliminaci√≥n de Gauss, considerando la triangulaci√≥n con pivoteo y la soluci√≥n de los dos sistemas triangulares, concluyendo que es un proceso muy estable, considerando que la matriz A no sea mal condicionada.

Una forma de chequear esto es controlando los elementos
de U, si crecen mucho es una se√±al de mala condici√≥n de
la misma

¬øC√≥mo afecta el numero de condicion a los tipos de m√©todos?

- En **m√©todos directos**, el mal condicionamiento afecta la **exactitud** de la soluci√≥n.
- En **m√©todos iterativos**, el mal condicionamiento afecta la **eficiencia y convergencia** del proceso.

## Comparaci√≥n entre metodos directos e iterativos

|                                 | **M√©todos Directos**           | **M√©todos Iterativos**                                               |
| ------------------------------- | ------------------------------ | -------------------------------------------------------------------- |
| **Tiempo de ejecuci√≥n**         | $\mathcal{O}(n^3)$             | $O(n^2 \times iteraciones)$                                          |
| **Almacenamiento**              | $n \times n$ (matriz completa) | $n$ (solo vectores y diagonales)                                     |
| **Errores de redondeo**         | Grandes                        | Despreciables (menos acumulativos)                                   |
| **Tiempo de ejecuci√≥n (total)** | Finito (se conoce a priori)    | Indeterminado (depende de la convergencia)                           |
| **Tareas adicionales (TI)**     | "Barato"                       | "Caro" (m√°s iteraciones o ajustes)                                   |
| **Aplicaciones t√≠picas**        | Problemas generales            | Problemas espec√≠ficos (matrices sparse, diagonales dominantes, etc.) |

---

# Unidad 4 - Interpolaci√≥n

## üîç Objetivo


Gran parte de las aproximaciones realizadas en An√°lisis Num√©rico consisten en aproximar una funci√≥n $f(x)$ desconocida, mediante una cierta funci√≥n $g(x)$, que se obtiene como combinaci√≥n de funciones, partiendo de alguna clase de funciones conocidas.

Existen distintos criterios para elegir los coeficientes, dando lugar a distintos tipos de aproximaci√≥n:
- Aproximaciones exactas o por interpolaci√≥n
- Aproximaciones por m√≠nimos cuadrados
- Aproximaciones de error m√≠nimo - m√°ximo


Dado un conjunto de puntos conocidos $(x_i, f(x_i))$, encontrar una funci√≥n que pase exactamente por ellos. La **interpolaci√≥n** permite:

- Estimar valores intermedios de una funci√≥n.
- Aproximar funciones complejas.
- Base para derivaci√≥n, integraci√≥n y resoluci√≥n de ecuaciones.


Familias de Funciones Bases: 
* Monomios
* Trigonometricas
* Splines (o a trozos)
* Exponenciales

## üî∏ Tipos de Interpolaci√≥n

- **Interpolaci√≥n Polin√≥mica** (global)
- **Interpolaci√≥n por tramos (Spline)**
- **Interpolaci√≥n de Newton / Lagrange**

---

## üü¢ Interpolaci√≥n Polin√≥mica


### üîß Forma general
Los polinomios son muy utilizados por su estabilidad. Dados $n+1$ puntos, existe un √∫nico polinomio de grado ‚â§ $n$ que los interpola:

$$
P_n(x) = a_0 + a_1 x + a_2 x^2 + \dots + a_n x^n
$$

El sistema se puede construir y resolver usando:

- **Forma de Vandermonde**
- **Forma de Lagrange**
- **Forma de Newton**

### ‚ö†Ô∏è Problemas (Chapra y Burden)

- Para muchos puntos ($n$ grande), el polinomio oscila fuertemente (**Fen√≥meno de Runge**).
- Poca estabilidad num√©rica si los puntos est√°n muy cerca.
- Mejor usar interpolaci√≥n por tramos.

---

## üü¢ Forma de Lagrange

### üîß F√≥rmula

$$
P_n(x) = \sum_{i=0}^{n} f(x_i) \cdot L_i(x)
$$

donde:

$$
L_i(x) = \prod_{\substack{j=0 \\ j \neq i}}^{n} \frac{x - x_j}{x_i - x_j}
$$

### üí° Ventajas

- No requiere resolver sistemas.

### ‚ö†Ô∏è Desventajas

- Requiere recalcular todo si se a√±ade un nuevo punto.
- No se reutiliza c√°lculo.
- Para estimar el error se requiere la derivada de orden $n+1$
- No es f√°cil de utilizar en problemas de integraci√≥n o diferenciaci√≥n

### üíª Complejidad

- Costo de $O(n^2)$

### üìâ Errores en la Interpolaci√≥n

$$
f(x) - P_n(x) = \frac{f^{(n+1)}(\xi)}{(n+1)!} \prod_{i=0}^{n}(x - x_i)
$$

- Crece con el n√∫mero de nodos si no son bien distribuidos

---

## üü¢ Forma de Newton (Diferencias Divididas)
La idea en este metodo es poder recalcular $P_n(x)$ reutilizando $P_{n-1}(x)$, es decir agregandole un termino de correcci√≥n $C(x)$ de grado $n$
### üîß Forma general

$$
P_n(x) = a_0 + a_1(x - x_0) + a_2(x - x_0)(x - x_1) + \dots
$$

Los coeficientes $a_k$ se calculan mediante **diferencias divididas**:

$$
f[x_i, x_{i+1}] = \frac{f(x_{i+1}) - f(x_i)}{x_{i+1} - x_i}
$$

$$
f[x_i, x_{i+1}, x_{i+2}] = \frac{f[x_{i+1}, x_{i+2}] - f[x_i, x_{i+1}]}{x_{i+2} - x_i}
$$

### üí° Ventajas

- Reutilizable si se agregan puntos.

### ‚ö†Ô∏è Desventajas, tipos de error:
#### Redondeo
- Datos: Si los valores dados est√°n aproximados (por mediciones o c√°lculos previos), arrastran errores
- Coeficientes: Cada diferencia dividida se calcula con restas y divisiones, que son muy sensibles a errores (sobre todo si los  $x_i$ est√°n cerca). Esto puede amplificar los errores de redondeo incluso si los datos eran buenos.
- Aproximaci√≥n: Al evaluar $P_n(x)$ para cierto valor de x, se hacen productos acumulativos como $(x-x_0)(x-x_1)\dots$ y cada operaci√≥n puede introducir peque√±os errores que se acumulan.
#### Truncamiento
Este es el error te√≥rico que aparece incluso sin redondeos. Representa cu√°nto se aleja el polinomio $P_n(x)$ de la funci√≥n real $f(x)$


### üíª Complejidad

- Costo de $O(n^2)$

### üìâ Errores en la Interpolaci√≥n: Regla del t√©rmino siguiente

$$
e_n = f[x_0,x_1,\dots,x_n,x_{n+1}](x-x_0)(x-x_1)...(x-x_n)
$$

- Crece con el n√∫mero de nodos si no son bien distribuidos

---
## üü¢ Interpolaci√≥n Polin√≥mica Segmentaria

Dados $n+1$ puntos $(x_0,y_0),(x_1,y_1),\dots,(x_n,y_n)$ con $ x_0\le x_1\le \dots \le x_n $ una funci√≥n spline de orden k (k-Spline) sobre dichos puntos es una funci√≥n S que verifica:

a. $S(x)=q_k(x)$ polinomio de grado $\le k$, $x\in[x_k,x_{k+1}]$,

b. $S(x_k) = y_k$

c. $S \in C^{k-1} [x_i,x_{i+1}]$


### üíπ Cubic Spline

Sea $ f(x): [a,b] \to \mathbb{R} $ y sean $ \{x_i\} $, $ i = 0,\dots,n $, $ n+1 $ puntos distintos en $[a,b]$, con $ a = x_0 < x_1 < x_2 < \dots < x_n = b $.

- **a)** En cada intervalo $ [x_i, x_{i+1}] $, $ S $ es un polinomio c√∫bico denotado por $ S_i(x) $.
- **b)** $ S_i(x_i) = f(x_i) $, para $ i = 0,\dots,n $
- **c)** $ S_{i+1}(x_{i+1}) = S_i(x_{i+1}) $
- **d)** $ S'_{i+1}(x_{i+1}) = S'_i(x_{i+1}) $
- **e)** $ S''_{i+1}(x_{i+1}) = S''_i(x_{i+1}) $
- **f)** Se satisface alguna de las siguientes condiciones de frontera:
  - $ S''(x_0) = S''(x_n) = 0 $ &nbsp; _(frontera libre o natural)_
  - $ S'(x_0) = f'(x_0) $ y $ S'(x_n) = f'(x_n) $ &nbsp; _(frontera sujeta)_


### üîß Objetivo

Construir polinomios c√∫bicos en cada intervalo $[x_i, x_{i+1}]$, garantizando **suavidad**: Continua en primera y segunda derivada.


### üìê Spline c√∫bico natural

- Condiciones de borde: $S''(x_0) = S''(x_n) = 0$
- Se resuelve un sistema tridiagonal ‚Üí **M√©todo de Thomas**

### üí° Ventajas

- Alta precisi√≥n y suavidad
- Evita oscilaciones del polinomio global

### üìâ Error 
Al usar una spline natural para interpolar una funci√≥n $f(x)$, el error es  proporcional a $h^4$ con $h = |x_i-x_{i+1}|$

---

## üìå Comparativa de M√©todos de Interpolaci√≥n

| M√©todo            | Tipo       | Reutilizable | Precisi√≥n | Estabilidad | Observaciones                                 |
| ----------------- | ---------- | ------------ | --------- | ----------- | --------------------------------------------- |
| **Lagrange**      | Global     | ‚ùå           | Alta      | Baja        | No se puede agregar puntos f√°cilmente         |
| **Newton DD**     | Global     | ‚úÖ           | Alta      | Media       | Buen desempe√±o incremental                    |
| **Spline c√∫bico** | Por tramos | ‚úÖ           | Muy alta  | Muy alta    | Suave en derivadas, requiere resolver sistema |

---

# Unidad 5 ‚Äì Integraci√≥n Num√©rica

## üîç Objetivo

Aproximar mediante una familia de funciones {$f_n(x), n\ge1$} que aproxime a $f(x)$ integrales definidas de la forma:

$$
I = \int_a^b f(x)\,dx
$$

cuando:

- $f(x)$ no tiene antiderivada elemental
- Solo se conoce $f(x)$ en puntos discretos
- Se desea una soluci√≥n aproximada con control del error

Usaremos **polinomios** como funciones de aproximacion 

### Considerando forma de Lagrange

$$
\int_a^b f(x)\, dx \cong \int_a^b  \sum_{i=0}^{n} f(x_i) \cdot l_i(x)  dx 
= \sum_{i=0}^{n} f(x_i) \int_a^b l_i(x)\, dx
$$


### **Suma de Cuadratura:**

$$
In(f) = \sum_{i=0}^{n} \alpha_i f(x_i), \quad x_i \in [a, b] \quad \forall i
$$

- $ \alpha_i $: coeficientes de cuadratura $ \left( \int_a^b l_i(x) \, dx \right) $
- $ x_i $: nodos de cuadratura

---

## üî∏ Tipos de M√©todos

1. **Reglas de Newton-Cotes**
Basados en interpolar $f(x)$ en puntos **equiespaciados** con un polinomio de grado $n$, y luego integrar ese polinomio.

* **Cerradas**: incluyen los extremos $a$ y $b$ como nodos
   - Regla del rect√°ngulo
   - Regla del trapecio
   - Regla de Simpson
* **Abiertas**: no incluyen $a$ ni $b$; usadas para integrales impropias o E.D.O.
Estas f√≥rmulas, en general no dan buenos resultados si [a,b] es grande

2. **Reglas compuestas:** aplican las anteriores por subintervalos y aplican la regla en cada tramo
3. **Cuadratura Gaussiana:** nodos y pesos √≥ptimos

---

### üü¢ Regla del Rect√°ngulo (orden 0)

$$
I \approx (b - a) \cdot f(a)
$$

üîπ **Error**:

$$
E = -\frac{(b-a)^2}{2} f'(\eta),\quad \eta \in (a, b)
$$

---

### üü¢ Regla del Trapecio (orden 1)

### üîß Aproximaci√≥n

Se interpola una recta entre $(a, f(a))$ y $(b, f(b))$:

$$
I \approx \frac{b - a}{2} \left[f(a) + f(b)\right]
$$

üîπ **Error**:

$$
E_T = -\frac{(b-a)^3}{12} f''(\eta)
$$


### üí° Ventajas
- Simple de implementar
- √ötil para funciones lineales o suavemente curvadas

## üíª Regla del Trapecio Compuesta

$$
I \approx \frac{h}{2} \left[ f(x_0) + 2\sum_{i=1}^{n-1} f(x_i) + f(x_n) \right]
$$

con $h = \frac{b-a}{n}$

$$
E_T^{(comp)} = -\frac{(b-a)h^2}{12} f''(\xi)
$$

---

### üü¢ Regla de Simpson (orden 2)

Requiere 3 puntos: $a, c = \frac{a+b}{2}, b$

$$
I \approx \frac{b-a}{6} \left[f(a) + 4f(c) + f(b)\right]
$$

üîπ **Error**:

$$
E_S = -\frac{(b-a)^5}{2880} f^{(4)}(\eta)
$$

## üíª Regla de Simpson Compuesta

Requiere **n par** subintervalos:

$$
I \approx \frac{h}{3} \left[ f(x_0) + 4\sum_{\text{impares}} f(x_i) + 2\sum_{\text{pares}} f(x_i) + f(x_n) \right]
$$

con $h = \frac{b-a}{n}$

$$
E_S^{(comp)} = -\frac{(b-a) h^4}{180} f^{(4)}(\eta)
$$

- Mucho m√°s preciso que el trapecio si $f$ es suave
- Tiene una precisi√≥n de tercer orden a√∫n cuando usa s√≥lo tres puntos, es decir, integra funciones hasta grado 3 de forma exacta

---
### üü© Casos con puntos no equiespaciados

Si $h_i = x_{i+1} - x_i$ var√≠a, se aplica la regla del trapecio individualmente en cada subintervalo:

$$
I \approx \sum_{i=0}^{n-1} \frac{h_i}{2} [f(x_i) + f(x_{i+1})]
$$

---
## Tabla comparativa de Newton-Cotes


| M√©todo                 | **F√≥rmula del M√©todo**                                                                                                                                                 | **F√≥rmula del Error Te√≥rico**                                   | **Orden** |
| ---------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------- | --------- |
| **$Trapecio$**    | $\frac{b-a}{2} \left[f(a) + f(b)\right]$                                                                                       | $\displaystyle E_T = -\frac{(b - a)^3}{12} f''(\xi)$            | $O(h^2)$  |
| **$Simpson$**     | $\frac{b-a}{6} \left[f(a) + 4f\left(\frac{a+b}{2}\right) + f(b)\right]$                                                        | $\displaystyle E_S = -\frac{(b - a)^5}{2880} f^{(4)}(\xi)$      | $O(h^4)$  |
| **$Trapecio_C$** | $\frac{h}{2} \left[ f(x_0) + 2\sum_{i=1}^{n-1} f(x_i) + f(x_n) \right]$                                 | $\displaystyle E_{T_c} = -\frac{(b - a) h^2}{12} f''(\xi)$      | $O(h^2)$  |
| **$Simpson_C$**  | $\frac{h}{3} \left[ f(x_0) + 4\sum_{i=1,\,\text{impar}}^{n-1} f(x_i) + 2\sum_{i=2,\,\text{par}}^{n-2} f(x_i) + f(x_n) \right]$ | $\displaystyle E_{S_c} = -\frac{(b - a) h^4}{180} f^{(4)}(\xi)$ | $O(h^4)$  |

Aclaracion: $h = \dfrac{b-a}{n}$
Observaciones: 
- Si la derivada correspondiente es peque√±a o constante, el m√©todo ser√° muy preciso incluso con pocos puntos.
- $¬øO(h^n)? \rightarrow$  A medida que $h$ se hace m√°s peque√±o, el error disminuye a una velocidad proporcional a $h^n$
- Los errores se pueden reducir usando subintervalos mas peque√±os

---

## üî∏ Extrapolaci√≥n de Richardson

Permite **mejorar una estimaci√≥n $I(h)$** con otra a paso $h/2$, realizando un promedio ponderado entre ambas:

$$
I_R = \frac{4I(h/2) - I(h)}{3}
$$

üîπ Se aplica sobre integrales aproximadas por trapecio (o Simpson)

üîπ Elimina el error de orden m√°s bajo ‚Üí mejora de $O(h^2)$ a $O(h^4)$

---

## üü¢ Integraci√≥n de Romberg

### üîß Idea

Aplica Richardson **recursivamente** para construir una tabla triangular, calculandose por filas:

$$
I_{j,k} = \frac{4^{k-1} I_{j,k-1} - I_{j-1,k-1}}{4^{k-1} - 1}
$$

üîπ Avanza en niveles hasta que el cambio entre niveles cumpla:

$$
|I_{k,k} - I_{k,k-1}| < \varepsilon
$$

![alt text]({74F5467D-9273-4755-8554-F079AA94C050}.png)

---

## üü¢ Cuadratura de Gauss
Las f√≥rmulas de cuadratura de Gauss se basan en buscar valores de $a_i$ y $x_i$ de forma tal que la aproximacion sea exacta para polinomios de grado lo mas alto posible.

### üîß Objetivo

Aproximar:

$$
\int_{-1}^{1} f(x)\,dx \approx \sum_{i=1}^{n} a_i f(x_i)
$$

donde:

- $x_i$: ra√≠ces del polinomio de Legendre de grado $n$
- $a_i$: pesos asociados

> Puede integrarse con exactitud polinomios de grado $2n-1$ con solo $n$ puntos si:
> - La formula es interpolatoria
> - Los nodos son las ra√≠ces del n-√©simo polinomio ortogonal en [a,b] (usamos polinomios de Legendre)

### üí° Ventajas

<!-- - Mucho m√°s precisa que Newton-Cotes con pocos puntos -->
- No requiere que los nodos est√©n equiespaciados
- √ötiles para integrales complicadas

### üíª Cambio de intervalo

Para transformar $[a,b]$ a $[-1,1]$:

$$
\int_a^b f(x)\,dx = \frac{b-a}{2} \int_{-1}^{1} f\left( \frac{b-a}{2}x + \frac{a+b}{2} \right)\,dx
$$

### üí° Ventajas y limitaciones

| Ventajas                        | Limitaciones                                            |
| ------------------------------- | ------------------------------------------------------- |
| Alta precisi√≥n con pocos puntos | Se requieren los valores de $f(x)$ en nodos espec√≠ficos |
| Ideal para funciones suaves     | No √∫til si solo se tienen datos tabulados               |
| M√°s eficiente que Newton-Cotes  | Dif√≠cil estimaci√≥n del error                            |

### Caracter√≠sticas
- Su mayor ventaja es la eficiencia en el c√°lculo, el doble de r√°pido que las de Newton Cotes
- Adem√°s permite calcular integrales con singularidades 
- Una limitaci√≥n de Cuadratura de Gauss es que debe evaluarse en puntos espec√≠ficos, es decir que debemos conocer la funci√≥n, lo cual muchas veces no ocurre cuando trabajamos con datos experimentales
- Es dif√≠cil de calcular su error


### üìò Tablas (para Gauss-Legendre)

Para $n = 2$:

$$
x_1 = -\frac{1}{\sqrt{3}}, \quad x_2 = \frac{1}{\sqrt{3}} \quad\text{y}\quad a_1 = a_2 = 1
$$

Para $n = 3$: 
$$
x_1 = -\sqrt{\frac{3}{5}}, \quad x_2 = 0, \quad x_3 = \sqrt{\frac{3}{5}} \quad\text{y}\quad a_1 =\frac{5}{9} , \quad a_2 = \frac{8}{9}, \quad a_3 = \frac{5}{9}
$$


## üìå Observaciones finales (Chapra/Burden)

- A mayor grado del polinomio, mayor el riesgo de oscilaciones ‚Üí usar con cuidado
- Cuadratura de Gauss es ideal cuando se busca **alta precisi√≥n con pocos puntos**