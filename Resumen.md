# Unidad 3 - Sistemas de ecuaciones lineales

## üîç Objetivo

Resolver sistemas del tipo:

$$
A \cdot x = b
$$

donde $A \in \mathbb{R}^{n \times n}$, $x$ es el vector inc√≥gnita y $b$ el vector de constantes. Se usan m√©todos num√©ricos para evitar errores acumulativos y reducir el costo computacional.

## üî∏ Tipos de Matrices

**DENSAS:** Son aquellas que poseen pocos elementos nulos y son de orden bajo\
**RALAS (SPARSE):** Son aquellas que poseen muchos ceros y son de orden alto

## üî∏ Tipos de M√©todos

---

### 1. **M√©todos Directos**

Son aquellos que nos conducir√≠an a la soluci√≥n exacta luego de un n√∫mero finito de operaciones elementales, si no hubiera errores de redondeo

### üü¢ **Eliminaci√≥n de Gauss (o Reducci√≥n Gaussiana) $O(n^3)$**

#### üîç Objetivo

Transforma el sistema en forma escalonada superior usando operaciones elementales a la matriz ampliada (modifica b) y luego se resuelve con **sustituci√≥n hacia atr√°s**.

Adem√°s, puede incluir **pivoteo parcial** para mejorar estabilidad num√©rica.

**Errores comunes:**

- El error de redondeo se acumula en cada paso, especialmente sin pivoteo.
- Matrices mal condicionadas pueden amplificar errores.

### üí° Ventajas

- M√©todo **general**: se puede aplicar a cualquier sistema no singular.
- Base de otros m√©todos como LU, Thomas (caso tridiagonal), y muchos algoritmos num√©ricos.
- Se puede mejorar con:

  - **Pivoteo parcial** (mayor estabilidad).
  - **Escalamiento** (evita errores por magnitudes muy diferentes).

### ‚ö†Ô∏è Requisitos

- La matriz debe ser **no singular**.
- Sin pivoteo, puede fallar o generar grandes errores num√©ricos si hay ceros o valores muy peque√±os en la diagonal.

### üíª Complejidad

- Eliminaci√≥n (reducci√≥n a triangular superior):$ O\left( \frac{2}{3} n^3 \right)$
- Sustituci√≥n hacia atr√°s:$ O(n^2) $
- **Total:** $O(n^3)$

---

### üü¢ **Factorizaci√≥n LU** $O(n^3)$

#### üîç Objetivo

Descompone $A = L \cdot U$, donde $A$ debe ser **no singular**, y para LU sin pivoteo, debe ser posible evitar ceros en la diagonal de $U$:

- $L$: matriz triangular inferior con unos en la diagonal
- $U$: matriz triangular superior
- Luego resuelve $L \cdot y = b$, y $U \cdot x = y$

#### üí° Ventajas

- √ötil para resolver muchos sistemas con la misma matriz $A$ y distintos vectores $b$.
- Se puede calcular $|A| = |L.U| = |U|$
- C√°lculo de $A^{-1}$ usando como b las columnas can√≥nicas
- Se puede combinar con **pivoteo parcial** para mayor estabilidad:

### üíª Complejidad

- **Factorizaci√≥n LU:** $ O\left( \frac{2}{3} n^3 \right) $
- **Sustituciones (hacia adelante y hacia atr√°s):** $ O(n^2) $
- **Total por un sistema:** $ O(n^3) $
- **Total si ya ten√©s la factorizaci√≥n (solo resolver para otro $b$):** $ O(n^2) $

### **Pivoteo parcial**

- Consiste en, en cada paso de la eliminaci√≥n de Gauss, buscar en la columna actual el elemento con mayor valor absoluto con el objetivo de evitar divisiones por n√∫meros muy peque√±os (o $0$) que aumentan errores de redondeo. **entre las filas restantes** (desde la fila pivote hacia abajo) e inicializar un vector $P$.

- **Vector $P$ (permutaciones):**
  Registra el orden actual de las filas luego de los intercambios durante la eliminaci√≥n. Inicialmente, $P = [1, 2, ..., n]$. Cada vez que se intercambian filas $i$ y $k$, se intercambian tambi√©n $P_i \leftrightarrow P_k$. √ötil para mantener la correspondencia entre filas originales y filas actuales, sin mover f√≠sicamente toda la matriz (optimizaci√≥n).

### **Escalamiento Implicito**

- Wilkinson propone que una matriz debe equilibrarse antes de aplicar una algoritmo de solucion. Se basa en **normalizar la comparaci√≥n de valores en cada fila seg√∫n la magnitud relativa en su fila** para elegir mejor el pivote. Por lo que para cada fila $i$, se calcula su factor de escala en $S$:

- **Vector $S$ (escalas):**
  Guarda para cada fila $i$ el valor m√°ximo absoluto de sus coeficientes en la matriz original $A$:

$$
S_i = \max_{1 \leq j \leq n} |a_{ij}|
$$

- Al elegir el pivote, no se mira solo el valor absoluto del elemento de la columna $k$, sino el cociente: $ c*i = \frac{|a*{ik}|}{S_i} $ haciendo una comparaci√≥n justa y relativa

### ‚öôÔ∏è Funcionamiento resumido del ppe:

1. Se calcula $S$ antes de comenzar la eliminaci√≥n y $P$ se inicializa desde 1 hasya n.
2. En cada paso $k$, para elegir fila pivote $p$, se eval√∫a: $ c*i = \frac{|a*{ik}|}{S_i}$

3. Se intercambian filas $k$ y $p$ tanto en la matriz como en $P$.
4. Se contin√∫a con la eliminaci√≥n habitual.
5. Al resolver queda: $PLU x = b \implies L y = P b \implies U x = y$. Es decir, debemos aplicar la permutacion a b.

#### üìù Beneficios

- Permite hacer pivoteo con una comparaci√≥n justa considerando la escala de cada fila.
- Mejora estabilidad y evita errores num√©ricos.
- El vector $P$ facilita reconstruir la soluci√≥n o aplicar permutaciones posteriores sin perder datos.

---

### üü¢ Descomposici√≥n de Cholesky

#### üîç Objetivo

Resolver $A x = b$ cuando $A$ es **sim√©trica y definida positiva**.

#### üìê Descomposici√≥n:

$$
A = L \cdot L^T
$$

- $L$: matriz triangular inferior con coeficientes reales, incluso en su diagonal.
- Luego se resuelven:

$$
L y = b \quad \text{(sustituci√≥n hacia adelante)}
$$

$$
L^T x = y \quad \text{(sustituci√≥n hacia atr√°s)}
$$

### üßÆ F√≥rmulas por componentes

Para construir $L$, se recorren filas y columnas de forma incremental. Los elementos se calculan as√≠:

#### üü© Diagonal ($i = j$):

$$
\ell_{ii} = \sqrt{ a_{ii} - \sum_{k=1}^{i-1} \ell_{ik}^2 }
$$

#### üü¶ Debajo de la diagonal ($i > j$):

$$
\ell_{ij} = \frac{1}{\ell_{jj}} \left( a_{ij} - \sum_{k=1}^{j-1} \ell_{ik} \ell_{jk} \right)
$$

### üí° Ventajas

- Es **m√°s eficiente** y **m√°s estable** que la LU tradicional si se cumplen las condiciones.
- Requiere casi la **mitad del trabajo** que LU.
- Ideal para problemas de ingenier√≠a con matrices sim√©tricas dispersas.

### üíª Complejidad

- Descomposici√≥n: $O\left( \frac{1}{3} n^3 \right)$ (mitad que LU)
- Sustituciones: $O(n^2)$

---

### üü¢ M√©todo de Thomas (tridiagonal)

#### üîç Objetivo

Resolver sistemas donde $A$ es **tridiagonal**, s√≥lo tiene elementos distintos de cero en la diagonal principal y las diagonales adyacentes, es decir:

$$
\begin{bmatrix}f_1 & g_1 & 0   & \cdots & 0 \\e_2 & f_2 & g_2 & \cdots & 0 \\0   & e_3 & f_3 & \ddots & \vdots \\\vdots & \ddots & \ddots & \ddots & g_{n-1} \\0 & \cdots & 0 & e_n & f_n\end{bmatrix} .
\begin{bmatrix}x_1 \\x_2 \\x_3 \\\vdots \\x_{n-1} \\x_n\end{bmatrix} =
\begin{bmatrix}r_1 \\r_2 \\r_3 \\\vdots \\r_{n-1} \\r_n\end{bmatrix}
$$

### üß† Algoritmo

#### Descomposici√≥n

Para $k = 2$ hasta $n$: $e_k = \frac{e_k}{f_{k-1}}$; $f*k = f_k - e_k \cdot g*{k-1} $

#### Sustituci√≥n hacia adelante

Para $k = 2$ hasta $n$: $r*k = r_k - e_k \cdot r*{k-1} $

#### Sustituci√≥n hacia atr√°s

$x_n = \frac{r_n}{f_n}$ \
Para $k = n-1$ hasta $1, -1$ : $x*k = \frac{r_k - g_k \cdot x*{k+1}}{f_k} $

Es una forma optimizada de **eliminaci√≥n de Gauss** que aprovecha la estructura tridiagonal para:

1. **Eliminar los elementos debajo de la diagonal** de forma eficiente
2. **Resolver con sustituci√≥n hacia atr√°s**

### üí° Ventajas

- No requiere almacenar toda la matriz ‚Üí solo 3 vectores
- Precisi√≥n superior a m√©todos gen√©ricos al reducir operaciones

### üíª Complejidad

- Fase de eliminaci√≥n: $O(n)$
- Fase de sustituci√≥n: $O(n)$
- **Total:** $O(n)$

---

### üìå Comparativa final de m√©todos directos

| M√©todo       | Requisitos principales                      | Complejidad                                    | Ventajas clave                                                           |
| ------------ | ------------------------------------------- | ---------------------------------------------- | ------------------------------------------------------------------------ |
| **Gauss**    | No singular y $a_{ii} \neq 0$ (sin pivoteo) | $O(n^3)$                                       | General, robusto, base de muchos otros m√©todos                           |
| **LU**       | No singular y $a_{ii} \neq 0$ (sin pivoteo) | $O(n^3)$ (una vez) <br> $O(n^2)$ (por sistema) | Reutilizable para varios $b$, m√°s eficiente que Gauss en ese caso        |
| **Cholesky** | **Sim√©trica definida positiva**             | $O\left(\frac{1}{3} n^3 \right)$               | M√°s r√°pido y estable que LU si aplica; usa menos operaciones             |
| **Thomas**   | **Tridiagonal**                             | $O(n)$                                         | Extremadamente eficiente; ideal para sistemas grandes con esa estructura |

---

# M√©todos Iterativos para Sistemas de Ecuaciones Lineales

## üîç Objetivo

Resolver sistemas lineales $A x = b$ mediante **aproximaciones sucesivas**, comenzando con una estimaci√≥n inicial $x^{(0)}$, y refinando la soluci√≥n en cada iteraci√≥n:

$$
x^{(k+1)} = G(x^{(k)})
$$

Se aplican principalmente cuando:

- El sistema es de **gran tama√±o** o **disperso (sparse)**.
- El almacenamiento o el costo computacional de los m√©todos directos es alto.
- Se desea **controlar el error** y limitar el n√∫mero de operaciones.

---

## üî∏ Tipos de M√©todos Iterativos

- **Jacobi**
- **Gauss-Seidel**
- **SOR (Successive Over-Relaxation)**

---

## üü¢ M√©todo de Jacobi

### üîß Ecuaciones base

Dado el sistema:

$$
A x = b \quad \Rightarrow \quad a_{ii} x_i^{(k+1)} = b_i - \sum_{j \neq i} a_{ij} x_j^{(k)}
$$

$$
x_i^{(k+1)} = \frac{1}{a_{ii}} \left( b_i - \sum_{j \neq i} a_{ij} x_j^{(k)} \right)
$$

> Cada componente se calcula **usando solo valores de la iteraci√≥n anterior**.

### üí° Ventajas

- Muy sencillo de implementar.
- F√°cil de paralelizar (cada ecuaci√≥n es independiente de las dem√°s en cada iteraci√≥n).
- √ötil para obtener una estimaci√≥n preliminar.

### ‚ö†Ô∏è Requisitos

- A debe ser **diagonalmente dominante** o **symmetric positive definite** para asegurar convergencia.
- La matriz no debe tener ceros en la diagonal.

### üíª Complejidad

- Cada iteraci√≥n: $O(n^2)$
- N√∫mero de iteraciones depende de la convergencia (condici√≥n del sistema).

### üìâ Errores y convergencia

- La convergencia est√° garantizada si la **norma del operador iterativo $G$** es menor que 1.
- El m√©todo **no asegura convergencia** para cualquier matriz.

---

## üü¢ M√©todo de Gauss-Seidel

### üîß Ecuaci√≥n de iteraci√≥n

$$
x_i^{(k+1)} = \frac{1}{a_{ii}} \left( b_i - \sum_{j < i} a_{ij} x_j^{(k+1)} - \sum_{j > i} a_{ij} x_j^{(k)} \right)
$$

> Se usa **inmediatamente** cada nuevo valor calculado $x_j^{(k+1)}$ para actualizar el siguiente.

### üí° Ventajas

- Mejora notablemente la velocidad de convergencia respecto a Jacobi.
- Menor consumo de memoria (no requiere vector auxiliar).
- Adecuado para sistemas **grandes y dispersos**.

### ‚ö†Ô∏è Requisitos

- Converge si A es **sim√©trica definida positiva** o **diagonalmente dominante**.
- No siempre converge si A tiene mala condici√≥n.

### üíª Complejidad

- Cada iteraci√≥n: $O(n^2)$
- Menos iteraciones que Jacobi (aunque m√°s dif√≠ciles de paralelizar).

### üìâ Observaciones (Chapra)

- En la pr√°ctica, se usa como base para aceleraci√≥n (como en SOR).
- M√°s susceptible a errores si hay malas estimaciones iniciales.

---

## üü¢ M√©todo SOR (Relajaci√≥n Sucesiva)

### üîß Ecuaci√≥n iterativa

$$
x_i^{(k+1)} = (1 - \omega) x_i^{(k)} + \frac{\omega}{a_{ii}} \left( b_i - \sum_{j < i} a_{ij} x_j^{(k+1)} - \sum_{j > i} a_{ij} x_j^{(k)} \right)
$$

- $\omega \in (0, 2)$: par√°metro de relajaci√≥n

  - $\omega = 1$ ‚Üí Gauss-Seidel
  - $\omega < 1$ ‚Üí subrelajaci√≥n
  - $\omega > 1$ ‚Üí **sobre-relajaci√≥n** (acelera convergencia si est√° bien elegido)

### üí° Ventajas

- Puede acelerar la convergencia de Gauss-Seidel de forma significativa.
- Muy √∫til en sistemas grandes y bien condicionados.

### ‚ö†Ô∏è Requisitos

- Mismo que Gauss-Seidel + elegir adecuadamente $\omega$
- Requiere prueba emp√≠rica o an√°lisis para encontrar el $\omega$ √≥ptimo.

### üíª Complejidad

- Cada iteraci√≥n: $O(n^2)$
- Menos iteraciones que Gauss-Seidel si $\omega$ es adecuado

### üìâ Observaciones (Chapra)

- En muchos problemas pr√°cticos, un $\omega \in [1.1, 1.5]$ acelera fuertemente la convergencia.
- Chapra sugiere experimentar con distintos valores y observar el n√∫mero de iteraciones requeridas.

---

## üìå Comparativa final de m√©todos iterativos

| M√©todo           | Requisitos para convergencia        | Convergencia | Paralelizaci√≥n | Velocidad relativa | Notas clave                          |
| ---------------- | ----------------------------------- | ------------ | -------------- | ------------------ | ------------------------------------ |
| **Jacobi**       | Diagonal dominante o sim√©trica p.d. | Lenta        | Muy f√°cil      | üü° Lenta           | Simple, pero puede diverger          |
| **Gauss-Seidel** | Diagonal dominante o sim√©trica p.d. | R√°pida       | Dif√≠cil        | üü¢ Media           | Usa valores actualizados al instante |
| **SOR**          | Idem + $\omega$ adecuado            | Muy r√°pida   | Dif√≠cil        | üü¢üü¢ R√°pida        | Acelera G-S si $\omega$ bien elegido |

---

¬°Perfecto! A continuaci√≥n te presento la **Unidad 5 ‚Äì Interpolaci√≥n**, en formato Markdown, siguiendo exactamente la misma estructura clara y detallada que usaste para los sistemas lineales. La informaci√≥n est√° basada en **Chapra** y **Burden**, incluyendo f√≥rmulas, propiedades, errores y comparativas donde corresponda.

---

# Unidad 4 Interpolaci√≥n

## üîç Objetivo

Dado un conjunto de puntos conocidos $(x_i, f(x_i))$, encontrar una funci√≥n que pase exactamente por ellos. La **interpolaci√≥n** permite:

- Estimar valores intermedios de una funci√≥n.
- Aproximar funciones complejas.
- Base para derivaci√≥n, integraci√≥n y resoluci√≥n de ecuaciones.

---

## üî∏ Tipos de Interpolaci√≥n

- **Interpolaci√≥n Polin√≥mica** (global)
- **Interpolaci√≥n por tramos (Spline)**
- **Interpolaci√≥n lineal y cuadr√°tica simple**
- **Interpolaci√≥n de Newton / Lagrange**

---

## üü¢ Interpolaci√≥n Polin√≥mica

### üîß Forma general

Dado $n+1$ puntos, existe un √∫nico polinomio de grado ‚â§ $n$ que los interpola:

$$
P_n(x) = a_0 + a_1 x + a_2 x^2 + \dots + a_n x^n
$$

El sistema se puede construir y resolver usando:

- **Forma de Vandermonde**
- **Forma de Lagrange**
- **Forma de Newton**

### ‚ö†Ô∏è Problemas (Chapra y Burden)

- Para muchos puntos ($n$ grande), el polinomio oscila fuertemente (**Fen√≥meno de Runge**).
- Poca estabilidad num√©rica si los puntos est√°n muy cerca.
- Mejor usar interpolaci√≥n por tramos o nodos Chebyshev.

---

## üü¢ Forma de Lagrange

### üîß F√≥rmula

$$
P_n(x) = \sum_{i=0}^{n} f(x_i) \cdot L_i(x)
$$

donde:

$$
L_i(x) = \prod_{\substack{j=0 \\ j \neq i}}^{n} \frac{x - x_j}{x_i - x_j}
$$

### üí° Ventajas

- No requiere resolver sistemas.
- Forma expl√≠cita del polinomio.

### ‚ö†Ô∏è Desventajas

- Requiere recalcular todo si se a√±ade un nuevo punto.
- No se reutiliza c√°lculo.

---

## üü¢ Forma de Newton (Diferencias Divididas)

### üîß Forma general

$$
P_n(x) = a_0 + a_1(x - x_0) + a_2(x - x_0)(x - x_1) + \dots
$$

Los coeficientes $a_k$ se calculan mediante **diferencias divididas**:

$$
f[x_i, x_{i+1}] = \frac{f(x_{i+1}) - f(x_i)}{x_{i+1} - x_i}
$$

$$
f[x_i, x_{i+1}, x_{i+2}] = \frac{f[x_{i+1}, x_{i+2}] - f[x_i, x_{i+1}]}{x_{i+2} - x_i}
$$

### üí° Ventajas

- Reutilizable si se agregan puntos.
- √ötil para tabulaci√≥n incremental.

---

## üü¢ Interpolaci√≥n Lineal y Cuadr√°tica

### üîß Interpolaci√≥n lineal

Usa dos puntos para construir una recta:

$$
f(x) \approx f(x_0) + \frac{f(x_1) - f(x_0)}{x_1 - x_0}(x - x_0)
$$

### üîß Interpolaci√≥n cuadr√°tica

Usa tres puntos para un polinomio de grado 2:

$$
P_2(x) = a_0 + a_1(x - x_0) + a_2(x - x_0)(x - x_1)
$$

Usando diferencias divididas.

---

## üü¢ Splines (Interpolaci√≥n por Tramos)

### üîß Objetivo

Construir polinomios de bajo grado (generalmente c√∫bicos) en cada intervalo $[x_i, x_{i+1}]$, garantizando **suavidad**:

- Contin√∫a en primera y segunda derivada
- Evita oscilaciones del polinomio global

### üìê Spline c√∫bico natural

- Condiciones de borde: $S''(x_0) = S''(x_n) = 0$
- Se resuelve un sistema tridiagonal ‚Üí **M√©todo de Thomas**

### üí° Ventajas

- Alta precisi√≥n y suavidad
- Muy usado en gr√°ficos, ingenier√≠a y simulaciones

---

## üìâ Errores en la Interpolaci√≥n

### üî∫ Error en interpolaci√≥n polin√≥mica (forma de Newton):

$$
f(x) - P_n(x) = \frac{f^{(n+1)}(\xi)}{(n+1)!} \prod_{i=0}^{n}(x - x_i)
$$

- Depende de derivada de orden $n+1$
- Crece con el n√∫mero de nodos si no son bien distribuidos

### üß† Recomendaci√≥n (Burden/Chapra)

- Usar **splines o polinomios de bajo grado por tramos** para alta precisi√≥n
- Evitar polinomios globales de grado alto

---

## üìå Comparativa de M√©todos de Interpolaci√≥n

| M√©todo            | Tipo       | Reutilizable | Precisi√≥n | Estabilidad | Observaciones                                 |
| ----------------- | ---------- | ------------ | --------- | ----------- | --------------------------------------------- |
| **Lagrange**      | Global     | ‚ùå           | Alta      | Baja        | No se puede agregar puntos f√°cilmente         |
| **Newton DD**     | Global     | ‚úÖ           | Alta      | Media       | Buen desempe√±o incremental                    |
| **Lineal**        | Por tramos | ‚úÖ           | Media     | Alta        | Muy simple, baja continuidad                  |
| **Cuadr√°tico**    | Por tramos | ‚úÖ           | Mejor     | Alta        | Aumenta suavidad y precisi√≥n                  |
| **Spline c√∫bico** | Por tramos | ‚úÖ           | Muy alta  | Muy alta    | Suave en derivadas, requiere resolver sistema |

---

¬øQuer√©s que prepare tambi√©n la parte de **nodos de Chebyshev**, o pasamos directamente a derivaci√≥n num√©rica?

¬°Perfecto! Continuamos con la **Unidad 6 ‚Äì Integraci√≥n Num√©rica**, en el mismo estilo que tus apuntes anteriores: bien estructurado, con f√≥rmulas claras, ventajas, errores, complejidad, y contenido basado en **Chapra** y **Burden**.

---

# Unidad 5 ‚Äì Integraci√≥n Num√©rica

## üîç Objetivo

Aproximar integrales definidas de la forma:

$$
I = \int_a^b f(x)\,dx
$$

cuando:

- $f(x)$ no tiene antiderivada elemental
- Solo se conoce $f(x)$ en puntos discretos
- Se desea una soluci√≥n aproximada con control del error

---

## üî∏ Tipos de M√©todos

1. **Reglas de Newton-Cotes (puntos equiespaciados):**

   - Regla del rect√°ngulo
   - Regla del trapecio
   - Regla de Simpson (1/3 y 3/8)

2. **Reglas compuestas:** aplican las anteriores por subintervalos

3. **Cuadratura Gaussiana:** nodos y pesos √≥ptimos

---

## üü¢ Regla del Trapecio

### üîß Aproximaci√≥n

Se interpola una recta entre $(a, f(a))$ y $(b, f(b))$:

$$
\int_a^b f(x)\,dx \approx \frac{b-a}{2} [f(a) + f(b)]
$$

### üí° Ventajas

- Simple de implementar
- √ötil para funciones lineales o suavemente curvadas

### ‚ö†Ô∏è Error

$$
E_T = -\frac{(b-a)^3}{12} f''(\xi)
$$

- Depende de la segunda derivada de $f$
- Se puede reducir dividiendo en subintervalos

## üíª Regla del Trapecio Compuesta

$$
\int_a^b f(x)\,dx \approx \frac{h}{2} \left( f(x_0) + 2\sum_{i=1}^{n-1} f(x_i) + f(x_n) \right)
$$

con $h = \frac{b-a}{n}$

$$
E_T^{(comp)} = -\frac{(b-a)h^2}{12} f''(\xi)
$$

---

## üü¢ Regla de Simpson 1/3

### üîß Aproximaci√≥n

Se interpola un polinomio cuadr√°tico entre 3 puntos:

$$
\int_a^b f(x)\,dx \approx \frac{b-a}{6} [f(a) + 4f\left( \frac{a+b}{2} \right) + f(b)]
$$

## üíª Regla de Simpson 1/3 Compuesta

Requiere **n par** subintervalos:

$$
\int_a^b f(x)\,dx \approx \frac{h}{3} \left[ f(x_0) + 4\sum_{\text{impares}} f(x_i) + 2\sum_{\text{pares}} f(x_i) + f(x_n) \right]
$$

### ‚ö†Ô∏è Error

$$
E_S = -\frac{(b-a)^5}{180} f^{(4)}(\xi)
$$

- Mucho m√°s preciso que el trapecio si $f$ es suave
- Usa segunda y cuarta derivada

---

## üü¢ Cuadratura de Gauss

### üîß Objetivo

Aproximar:

$$
\int_{-1}^{1} f(x)\,dx \approx \sum_{i=1}^{n} w_i f(x_i)
$$

donde:

- $x_i$: ra√≠ces del polinomio de Legendre de grado $n$
- $w_i$: pesos asociados

> Puede integrarse con exactitud polinomios de grado $2n-1$ con solo $n$ puntos.

### üí° Ventajas

- Mucho m√°s precisa que Newton-Cotes con pocos puntos
- No requiere que los nodos est√©n equiespaciados
- Chapra y Burden recomiendan para integrales complicadas

### üíª Cambio de intervalo

Para transformar $[a,b]$ a $[-1,1]$:

$$
\int_a^b f(x)\,dx = \frac{b-a}{2} \int_{-1}^{1} f\left( \frac{b-a}{2}x + \frac{a+b}{2} \right)\,dx
$$

### üìò Tablas (para Gauss-Legendre)

Para $n = 2$:

$$
x_1 = -\frac{1}{\sqrt{3}}, \quad x_2 = \frac{1}{\sqrt{3}} \quad\text{y}\quad w_1 = w_2 = 1
$$

Para $n = 4$, los nodos y pesos son fracciones con ra√≠ces cuadradas (ver tabla en apunte anterior).

---

## üìâ Comparativa de M√©todos

| M√©todo             | Orden  | Nodos equiespaciados | Precisi√≥n relativa | Observaciones         |
| ------------------ | ------ | -------------------- | ------------------ | --------------------- |
| Trapecio simple    | 2      | ‚úÖ                   | Baja               | Sencillo              |
| Trapecio compuesto | 2      | ‚úÖ                   | Media              | Mejora con n          |
| Simpson 1/3        | 4      | ‚úÖ (n par)           | Alta               | Muy usado             |
| Simpson 3/8        | 4      | ‚úÖ (n m√∫ltiplo de 3) | Similar a 1/3      | Poco m√°s complejo     |
| Gauss-Legendre     | $2n-1$ | ‚ùå                   | Muy alta           | Nodos y pesos √≥ptimos |

---

## üìå Observaciones finales (Chapra/Burden)

- A mayor grado del polinomio, mayor el riesgo de oscilaciones ‚Üí usar con cuidado
- M√©todos compuestos (Simpson/Trapecio) son preferibles a globales
- Cuadratura de Gauss es ideal cuando se busca **alta precisi√≥n con pocos puntos**
- Siempre tener en cuenta el comportamiento de las derivadas al estimar el error
