# Unidad 3 - Sistemas de ecuaciones lineales

## ğŸ” Objetivo

Resolver sistemas del tipo:

$$
A \cdot x = b
$$

donde $A \in \mathbb{R}^{n \times n}$, $x$ es el vector incÃ³gnita y $b$ el vector de constantes. Se usan mÃ©todos numÃ©ricos para evitar errores acumulativos y reducir el costo computacional.

## ğŸ”¸ Tipos de Matrices

**DENSAS:** Son aquellas que poseen pocos elementos nulos y son de orden bajo\
**RALAS (SPARSE):** Son aquellas que poseen muchos ceros y son de orden alto

## ğŸ”¸ Tipos de MÃ©todos

### 1. **MÃ©todos Directos**

Son aquellos que nos conducirÃ­an a la soluciÃ³n exacta luego de un nÃºmero finito de operaciones elementales, si no hubiera errores de redondeo

### ğŸŸ¢ **EliminaciÃ³n de Gauss**

#### ğŸ” Objetivo

Transforma el sistema en forma escalonada superior usando operaciones elementales a la matriz ampliada (modifica b) y luego se resuelve con **sustituciÃ³n hacia atrÃ¡s**.

**Errores comunes:**

- El error de redondeo se acumula en cada paso.
- Matrices mal condicionadas pueden amplificar errores.

### ğŸ’¡ Ventajas

- MÃ©todo **general**: se puede aplicar a cualquier sistema no singular.
- Base de otros mÃ©todos como LU, Thomas (caso tridiagonal), y muchos algoritmos numÃ©ricos.
- Se puede mejorar con:

  - **Pivoteo parcial** (mayor estabilidad).
  - **Escalamiento** (evita errores por magnitudes muy diferentes).

### âš ï¸ Requisitos

- La matriz debe ser **no singular**.
- Sin pivoteo, puede fallar o generar grandes errores numÃ©ricos si hay ceros o valores muy pequeÃ±os en la diagonal.

### ğŸ’» Complejidad

- EliminaciÃ³n (reducciÃ³n a triangular superior):$ O\left( \frac{2}{3} n^3 \right)$
- SustituciÃ³n hacia atrÃ¡s:$ O(n^2) $
- **Total:** $O(n^3)$

---

### ğŸŸ¢ **FactorizaciÃ³n LU**

#### ğŸ” Objetivo

Descompone $A = L \cdot U$, donde $A$ debe ser **no singular**, y para LU sin pivoteo, debe ser posible evitar ceros en la diagonal de $U$:

- $L$: matriz triangular inferior con unos en la diagonal
- $U$: matriz triangular superior
- Luego resuelve $L \cdot y = b$, y $U \cdot x = y$

#### ğŸ’¡ Ventajas

- Ãštil para resolver muchos sistemas con la misma matriz $A$ y distintos vectores $b$.
- Se puede calcular $|A| = |L.U| = |U|$
- CÃ¡lculo de $A^{-1}$ usando como b las columnas canÃ³nicas
- Se puede combinar con **pivoteo parcial escalado** para mayor estabilidad

### ğŸ’» Complejidad

- **FactorizaciÃ³n LU:** $ O\left( \frac{2}{3} n^3 \right) $
- **Sustituciones (hacia adelante y hacia atrÃ¡s):** $ O(n^2) $
- **Total por un sistema:** $ O(n^3) $
- **Total si ya tenÃ©s la factorizaciÃ³n (solo resolver para otro $b$):** $ O(n^2) $

### **Pivoteo parcial**

- Consiste en, en cada paso de la eliminaciÃ³n de Gauss, buscar en la columna actual **entre las filas restantes** (desde la fila pivote hacia abajo) el elemento con mayor valor absoluto con el objetivo de evitar divisiones por nÃºmeros muy pequeÃ±os (o $0$) de tal forma que los multiplicadores $|m_{ik}| \leq 1$, evitando aumentar errores de redondeo.
- **Vector $P$ (permutaciones):**
  Registra el orden actual de las filas luego de los intercambios durante la eliminaciÃ³n. Inicialmente, $P = [1, 2, ..., n]^t$. Cada vez que se intercambian filas $i$ y $k$, se intercambian tambiÃ©n $P_i \leftrightarrow P_k$. Ãštil para mantener la correspondencia entre filas originales y filas actuales, sin mover fÃ­sicamente toda la matriz (optimizaciÃ³n).

### **Escalamiento Implicito**

- Wilkinson propone que una matriz debe equilibrarse antes de aplicar una algoritmo de solucion. Se basa en **normalizar la comparaciÃ³n de valores en cada fila segÃºn la magnitud relativa en su fila** para elegir mejor el pivote. Por lo que para cada fila $i$, se calcula su factor de escala en $S$:

- **Vector $S$ (escalas):**
  Guarda para cada fila $i$ el valor mÃ¡ximo absoluto de sus coeficientes en la matriz original $A$:

$$
S_i = \max_{1 \leq j \leq n} |a_{ij}|
$$

- Al elegir el pivote, no se mira solo el valor absoluto del elemento de la columna $k$, sino el cociente: $ ck =\max{1 \leq j \leq n} \frac{|a\_{ij}^k| }{S_i} $ haciendo una comparaciÃ³n justa y relativa

### âš™ï¸ Funcionamiento resumido del ppe:

1. Se calcula $S$ antes de comenzar la eliminaciÃ³n y $P$ se inicializa desde 1 hasya n.
2. En cada paso $k$, para elegir fila pivote $p$, se evalÃºa: $ c*i = \frac{|a*{ik}|}{S_i}$

3. Se intercambian filas $k$ y $p$ tanto en la matriz como en $P$.
4. Se continÃºa con la eliminaciÃ³n habitual.
5. Al resolver queda: $PLU x = b \implies L y = P b \implies U x = y$. Es decir, debemos aplicar la permutacion a b.

#### ğŸ“ Beneficios

- Permite hacer pivoteo con una comparaciÃ³n justa considerando la escala de cada fila.
- Mejora estabilidad y evita errores numÃ©ricos.
- El vector $P$ facilita reconstruir la soluciÃ³n o aplicar permutaciones posteriores sin perder datos.

---

### ğŸŸ¢ Metodo de Gauss Jordan

Es similar al mÃ©todo de Gauss, la diferencia es que se diagonaliza la matriz

Al finalizar el algoritmo tenemos $ğ‘¥ = b^n$

Desventaja: Costo aumenta en 50%

### ğŸŸ¢ DescomposiciÃ³n de Cholesky

#### ğŸ” Objetivo

Resolver $A x = b$ cuando $A$ es **simÃ©trica y definida positiva**.

#### ğŸ“ DescomposiciÃ³n:

$$
A = L \cdot L^T
$$

- $L$: matriz triangular inferior con coeficientes reales, incluso en su diagonal.
- Luego se resuelven:

$$
L y = b \quad \text{(sustituciÃ³n hacia adelante)}
$$

$$
L^T x = y \quad \text{(sustituciÃ³n hacia atrÃ¡s)}
$$

### ğŸ§® FÃ³rmulas por componentes

Para construir $L$, se recorren filas y columnas de forma incremental. Los elementos se calculan asÃ­:

#### ğŸŸ© Diagonal ($i = j$):

$$
\ell_{ii} = \sqrt{ a_{ii} - \sum_{k=1}^{i-1} \ell_{ik}^2 }
$$

#### ğŸŸ¦ Debajo de la diagonal ($i > j$):

$$
\ell_{ji} = \frac{1}{\ell_{ii}} \left( a_{ji} - \sum_{k=1}^{i-1} \ell_{jk} \ell_{ik} \right)
$$

### ğŸ’¡ Ventajas

- Es **mÃ¡s eficiente** y **mÃ¡s estable** que la LU tradicional si se cumplen las condiciones.
- Requiere casi la **mitad del trabajo** que LU.
- No necesita pivoteo
- Ideal para matrices simÃ©tricas dispersas.

### ğŸ’» Complejidad

- DescomposiciÃ³n: $O\left( \frac{1}{3} n^3 \right)$ (mitad que LU)
- Sustituciones: $O(n^2)$

---

### ğŸŸ¢ MÃ©todo de Thomas (tridiagonal)

#### ğŸ” Objetivo

Resolver sistemas donde $A$ es **tridiagonal**, sÃ³lo tiene elementos distintos de cero en la diagonal principal y las diagonales adyacentes, es decir:

$$
\begin{bmatrix}f_1 & g_1 & 0   & \cdots & 0 \\e_2 & f_2 & g_2 & \cdots & 0 \\0   & e_3 & f_3 & \ddots & \vdots \\\vdots & \ddots & \ddots & \ddots & g_{n-1} \\0 & \cdots & 0 & e_n & f_n\end{bmatrix} .
\begin{bmatrix}x_1 \\x_2 \\x_3 \\\vdots \\x_{n-1} \\x_n\end{bmatrix} =
\begin{bmatrix}r_1 \\r_2 \\r_3 \\\vdots \\r_{n-1} \\r_n\end{bmatrix}
$$

### ğŸ§  Algoritmo

#### DescomposiciÃ³n

Para $k = 2$ hasta $n$: $e_k = \frac{e_k}{f_{k-1}}$; $f_k = f_k - e_k \cdot g_{k-1} $

#### SustituciÃ³n hacia adelante

Para $k = 2$ hasta $n$: $r*k = r_k - e_k \cdot r*{k-1} $

#### SustituciÃ³n hacia atrÃ¡s

$x_n = \frac{r_n}{f_n}$ \
Para $k = n-1$ hasta $1, -1$ : $x_k = \frac{r_k - g_k \cdot x_{k+1}}{f_k} $

Es una forma optimizada de **eliminaciÃ³n de Gauss** que aprovecha la estructura tridiagonal para:

1. **Eliminar los elementos debajo de la diagonal** de forma eficiente
2. **Resolver con sustituciÃ³n hacia atrÃ¡s**

### ğŸ’¡ Ventajas

- No requiere almacenar toda la matriz â†’ solo 3 vectores
- PrecisiÃ³n superior a mÃ©todos genÃ©ricos al reducir operaciones

### ğŸ’» Complejidad

- Fase de eliminaciÃ³n: $O(n)$
- Fase de sustituciÃ³n: $O(n)$
- **Total:** $O(n)$

---

### ğŸ“Œ Comparativa final de mÃ©todos directos

| MÃ©todo       | Requisitos principales                      | Complejidad                                    | Ventajas clave                                                           |
| ------------ | ------------------------------------------- | ---------------------------------------------- | ------------------------------------------------------------------------ |
| **Gauss**    | No singular y $a_{ii} \neq 0$ (sin pivoteo) | $O(n^3)$                                       | General, robusto, base de muchos otros mÃ©todos                           |
| **LU**       | No singular y $a_{ii} \neq 0$ (sin pivoteo) | $O(n^3)$ (una vez) <br> $O(n^2)$ (por sistema) | Reutilizable para varios $b$, mÃ¡s eficiente que Gauss en ese caso        |
| **Cholesky** | **SimÃ©trica definida positiva**             | $O\left(\frac{1}{3} n^3 \right)$               | MÃ¡s rÃ¡pido y estable que LU si aplica; usa menos operaciones             |
| **Thomas**   | **Tridiagonal**                             | $O(n)$                                         | Extremadamente eficiente; ideal para sistemas grandes con esa estructura |

---

### 2. Metodos iterativos

## ğŸ” Objetivo

Resolver sistemas lineales $A x = b$ mediante **aproximaciones sucesivas**, comenzando con una estimaciÃ³n inicial $x^{(0)}$, que en principio convergen a la solucion x:

$$
x^{(k+1)} = B(x^{(k)}) + C
$$

B se llama matriz de iteraciÃ³n, es una generalizaciÃ³n del mÃ©todo de punto fijo.

Se aplican principalmente cuando el sistema es de **gran tamaÃ±o** o **disperso (sparse)**.

### ğŸ’¡ Ventajas respecto a los metodos directos

- NÃºmero de operaciones
- Posiciones de memoria
- Errores de redondeo

## ğŸ”¸ Tipos de MÃ©todos Iterativos

- **Jacobi**
- **Gauss-Seidel**
- **SOR (Successive Over-Relaxation)**

---

## ğŸŸ¢ MÃ©todo de Jacobi

### ğŸ”§ Ecuaciones base

Dado el sistema:

$$
A x = b \quad \Rightarrow \quad a_{ii} x_i^{(k+1)} = b_i - \sum_{j \neq i} a_{ij} x_j^{(k)}
$$

$$
x_i^{(k+1)} = \frac{1}{a_{ii}} \left( b_i - \sum_{j \neq i} a_{ij} x_j^{(k)} \right)
$$

> Cada componente se calcula **usando solo valores de la iteraciÃ³n anterior** $\rightarrow$ Aproximaciones simultaneas.

### ğŸ’¡ Ventajas

- Muy sencillo de implementar.
- FÃ¡cil de paralelizar (cada ecuaciÃ³n es independiente de las demÃ¡s en cada iteraciÃ³n).

### âš ï¸ Requisitos

- La matriz no debe tener ceros en la diagonal.

### ğŸ’» Complejidad

- Cada iteraciÃ³n: $O(n^2)$
- NÃºmero de iteraciones depende de la convergencia (condiciÃ³n del sistema).

---

## ğŸŸ¢ MÃ©todo de Gauss-Seidel

### ğŸ”§ EcuaciÃ³n de iteraciÃ³n

$$
x_i^{(k+1)} = \frac{1}{a_{ii}} \left( b_i - \sum_{j < i} a_{ij} x_j^{(k+1)} - \sum_{j > i} a_{ij} x_j^{(k)} \right)
$$

> Se usa **inmediatamente** cada nuevo valor calculado $x_j^{(k+1)}$ para actualizar el siguiente.

### ğŸ’¡ Ventajas

- Mejora notablemente la velocidad de convergencia respecto a Jacobi.
- Menor consumo de memoria (no requiere vector auxiliar).
- Adecuado para sistemas **grandes y dispersos**.

### âš ï¸ Requisitos

- No siempre converge si A tiene mala condiciÃ³n.

### ğŸ’» Complejidad

- Cada iteraciÃ³n: $O(n^2)$
- Menos iteraciones que Jacobi (aunque mÃ¡s difÃ­ciles de paralelizar).

### ğŸ“‰ Observaciones (Chapra)

- En la prÃ¡ctica, se usa como base para aceleraciÃ³n (como en SOR).
- MÃ¡s susceptible a errores si hay malas estimaciones iniciales.

---

## ğŸŸ¢ MÃ©todo SOR (RelajaciÃ³n Sucesiva)

Este mÃ©todo usa un factor de ponderaciÃ³n para mejorar el valor calculado.

### ğŸ”§ EcuaciÃ³n iterativa

$$
x_i^{(k+1)} = (1 - \omega) x_i^{(k)} + \frac{\omega}{a_{ii}} \left( b_i - \sum_{j < i} a_{ij} x_j^{(k+1)} - \sum_{j > i} a_{ij} x_j^{(k)} \right)
$$

- $\omega \in (0, 2)$: parÃ¡metro de relajaciÃ³n

  - $\omega = 1$ â†’ Gauss-Seidel
  - $\omega < 1$ â†’ subrelajaciÃ³n
  - $\omega > 1$ â†’ **sobre-relajaciÃ³n** (acelera convergencia si estÃ¡ bien elegido)

### ğŸ’¡ Ventajas

- Puede acelerar la convergencia de Gauss-Seidel de forma significativa.
- Muy Ãºtil en sistemas grandes y bien condicionados.

### âš ï¸ Requisitos

- Requiere prueba empÃ­rica o anÃ¡lisis para encontrar el $\omega$ Ã³ptimo.

### ğŸ’» Complejidad

- Cada iteraciÃ³n: $O(n^2)$
- Menos iteraciones que Gauss-Seidel si $\omega$ es adecuado

### ğŸ“‰ Observaciones (Chapra)

- En muchos problemas prÃ¡cticos, un $\omega \in [1.1, 1.5]$ acelera fuertemente la convergencia.
- Chapra sugiere experimentar con distintos valores y observar el nÃºmero de iteraciones requeridas.

## ğŸ“Œ Comparativa final de mÃ©todos iterativos

| MÃ©todo           | Requisitos para convergencia                      | Convergencia | ParalelizaciÃ³n | Velocidad relativa | Notas clave                          |
| ---------------- | ------------------------------------------------- | ------------ | -------------- | ------------------ | ------------------------------------ |
| **Jacobi**       | Diagonal dominante                                | Lenta        | Muy fÃ¡cil      | ğŸŸ¡ Lenta           | Simple, pero puede diverger          |
| **Gauss-Seidel** | Diagonal dominante, simÃ©trica y definida positiva | RÃ¡pida       | DifÃ­cil        | ğŸŸ¢ Media           | Usa valores actualizados al instante |
| **SOR**          | Idem + $\omega$ adecuado                          | Muy rÃ¡pida   | DifÃ­cil        | ğŸŸ¢ğŸŸ¢ RÃ¡pida        | Acelera G-S si $\omega$ bien elegido |

---

## NÃºmero de condiciÃ³n

$K(A) = ||A|| \cdot ||A^{-1}||$

- Es un medida cuantitativa del grado de mal condicionamiento de la matriz de coeficientes. Mide cuan cerca estÃ¡ una matriz de ser singular
- Se usa para calcular como afectan los errores relativos en A y/o b el cÃ¡lculo de x.
- Si A y b tienen t cifras significativas y Îº(A) es de un orden 10 s entonces la precisiÃ³n del resultado serÃ¡ 10 s-t
- Se puede demostrar que:

  $$\frac {||\delta x||} {||x||} \le K(A) \frac {||\delta b||}{||b||} $$
  $$\frac {||\delta x||} {||x + \delta x||} \le K(A) \frac {||\delta A||}{||A||} $$

Wilkinson estudiÃ³ el efecto del redondeo en el mÃ©todo de EliminaciÃ³n de Gauss, considerando la triangulaciÃ³n con pivoteo y la soluciÃ³n de los dos sistemas triangulares, concluyendo que es un proceso muy estable, considerando que la matriz A no sea mal condicionada.

Una forma de chequear esto es controlando los elementos
de U, si crecen mucho es una seÃ±al de mala condiciÃ³n de
la misma

Â¿CÃ³mo afecta el numero de condicion a los tipos de mÃ©todos?

- En **mÃ©todos directos**, el mal condicionamiento afecta la **exactitud** de la soluciÃ³n.
- En **mÃ©todos iterativos**, el mal condicionamiento afecta la **eficiencia y convergencia** del proceso.

## ComparaciÃ³n entre metodos directos e iterativos

|                                 | **MÃ©todos Directos**           | **MÃ©todos Iterativos**                                               |
| ------------------------------- | ------------------------------ | -------------------------------------------------------------------- |
| **Tiempo de ejecuciÃ³n**         | $\mathcal{O}(n^3)$             | $O(n^2 \times iteraciones)$                                          |
| **Almacenamiento**              | $n \times n$ (matriz completa) | $n$ (solo vectores y diagonales)                                     |
| **Errores de redondeo**         | Grandes                        | Despreciables (menos acumulativos)                                   |
| **Tiempo de ejecuciÃ³n (total)** | Finito (se conoce a priori)    | Indeterminado (depende de la convergencia)                           |
| **Tareas adicionales (TI)**     | "Barato"                       | "Caro" (mÃ¡s iteraciones o ajustes)                                   |
| **Aplicaciones tÃ­picas**        | Problemas generales            | Problemas especÃ­ficos (matrices sparse, diagonales dominantes, etc.) |

---

# Unidad 4 InterpolaciÃ³n

## ğŸ” Objetivo

Dado un conjunto de puntos conocidos $(x_i, f(x_i))$, encontrar una funciÃ³n que pase exactamente por ellos. La **interpolaciÃ³n** permite:

- Estimar valores intermedios de una funciÃ³n.
- Aproximar funciones complejas.
- Base para derivaciÃ³n, integraciÃ³n y resoluciÃ³n de ecuaciones.

## ğŸ”¸ Tipos de InterpolaciÃ³n

- **InterpolaciÃ³n PolinÃ³mica** (global)
- **InterpolaciÃ³n por tramos (Spline)**
- **InterpolaciÃ³n lineal y cuadrÃ¡tica simple**
- **InterpolaciÃ³n de Newton / Lagrange**

---

## ğŸŸ¢ InterpolaciÃ³n PolinÃ³mica

### ğŸ”§ Forma general

Dado $n+1$ puntos, existe un Ãºnico polinomio de grado â‰¤ $n$ que los interpola:

$$
P_n(x) = a_0 + a_1 x + a_2 x^2 + \dots + a_n x^n
$$

El sistema se puede construir y resolver usando:

- **Forma de Vandermonde**
- **Forma de Lagrange**
- **Forma de Newton**

### âš ï¸ Problemas (Chapra y Burden)

- Para muchos puntos ($n$ grande), el polinomio oscila fuertemente (**FenÃ³meno de Runge**).
- Poca estabilidad numÃ©rica si los puntos estÃ¡n muy cerca.
- Mejor usar interpolaciÃ³n por tramos o nodos Chebyshev.

---

## ğŸŸ¢ Forma de Lagrange

### ğŸ”§ FÃ³rmula

$$
P_n(x) = \sum_{i=0}^{n} f(x_i) \cdot L_i(x)
$$

donde:

$$
L_i(x) = \prod_{\substack{j=0 \\ j \neq i}}^{n} \frac{x - x_j}{x_i - x_j}
$$

### ğŸ’¡ Ventajas

- No requiere resolver sistemas.
- Forma explÃ­cita del polinomio.

### âš ï¸ Desventajas

- Requiere recalcular todo si se aÃ±ade un nuevo punto.
- No se reutiliza cÃ¡lculo.

---

## ğŸŸ¢ Forma de Newton (Diferencias Divididas)

### ğŸ”§ Forma general

$$
P_n(x) = a_0 + a_1(x - x_0) + a_2(x - x_0)(x - x_1) + \dots
$$

Los coeficientes $a_k$ se calculan mediante **diferencias divididas**:

$$
f[x_i, x_{i+1}] = \frac{f(x_{i+1}) - f(x_i)}{x_{i+1} - x_i}
$$

$$
f[x_i, x_{i+1}, x_{i+2}] = \frac{f[x_{i+1}, x_{i+2}] - f[x_i, x_{i+1}]}{x_{i+2} - x_i}
$$

### ğŸ’¡ Ventajas

- Reutilizable si se agregan puntos.
- Ãštil para tabulaciÃ³n incremental.

---

## ğŸŸ¢ InterpolaciÃ³n Lineal y CuadrÃ¡tica

### ğŸ”§ InterpolaciÃ³n lineal

Usa dos puntos para construir una recta:

$$
f(x) \approx f(x_0) + \frac{f(x_1) - f(x_0)}{x_1 - x_0}(x - x_0)
$$

### ğŸ”§ InterpolaciÃ³n cuadrÃ¡tica

Usa tres puntos para un polinomio de grado 2:

$$
P_2(x) = a_0 + a_1(x - x_0) + a_2(x - x_0)(x - x_1)
$$

Usando diferencias divididas.

---

## ğŸŸ¢ Splines (InterpolaciÃ³n por Tramos)

### ğŸ”§ Objetivo

Construir polinomios de bajo grado (generalmente cÃºbicos) en cada intervalo $[x_i, x_{i+1}]$, garantizando **suavidad**:

- ContinÃºa en primera y segunda derivada
- Evita oscilaciones del polinomio global

### ğŸ“ Spline cÃºbico natural

- Condiciones de borde: $S''(x_0) = S''(x_n) = 0$
- Se resuelve un sistema tridiagonal â†’ **MÃ©todo de Thomas**

### ğŸ’¡ Ventajas

- Alta precisiÃ³n y suavidad
- Muy usado en grÃ¡ficos, ingenierÃ­a y simulaciones

---

## ğŸ“‰ Errores en la InterpolaciÃ³n

### ğŸ”º Error en interpolaciÃ³n polinÃ³mica (forma de Newton):

$$
f(x) - P_n(x) = \frac{f^{(n+1)}(\xi)}{(n+1)!} \prod_{i=0}^{n}(x - x_i)
$$

- Depende de derivada de orden $n+1$
- Crece con el nÃºmero de nodos si no son bien distribuidos

### ğŸ§  RecomendaciÃ³n (Burden/Chapra)

- Usar **splines o polinomios de bajo grado por tramos** para alta precisiÃ³n
- Evitar polinomios globales de grado alto

---

## ğŸ“Œ Comparativa de MÃ©todos de InterpolaciÃ³n

| MÃ©todo            | Tipo       | Reutilizable | PrecisiÃ³n | Estabilidad | Observaciones                                 |
| ----------------- | ---------- | ------------ | --------- | ----------- | --------------------------------------------- |
| **Lagrange**      | Global     | âŒ           | Alta      | Baja        | No se puede agregar puntos fÃ¡cilmente         |
| **Newton DD**     | Global     | âœ…           | Alta      | Media       | Buen desempeÃ±o incremental                    |
| **Lineal**        | Por tramos | âœ…           | Media     | Alta        | Muy simple, baja continuidad                  |
| **CuadrÃ¡tico**    | Por tramos | âœ…           | Mejor     | Alta        | Aumenta suavidad y precisiÃ³n                  |
| **Spline cÃºbico** | Por tramos | âœ…           | Muy alta  | Muy alta    | Suave en derivadas, requiere resolver sistema |

---

# Unidad 5 â€“ IntegraciÃ³n NumÃ©rica

## ğŸ” Objetivo

Aproximar integrales definidas de la forma:

$$
I = \int_a^b f(x)\,dx
$$

cuando:

- $f(x)$ no tiene antiderivada elemental
- Solo se conoce $f(x)$ en puntos discretos
- Se desea una soluciÃ³n aproximada con control del error

---

## ğŸ”¸ Tipos de MÃ©todos

1. **Reglas de Newton-Cotes (puntos equiespaciados):**

   - Regla del rectÃ¡ngulo
   - Regla del trapecio
   - Regla de Simpson (1/3 y 3/8)

2. **Reglas compuestas:** aplican las anteriores por subintervalos

3. **Cuadratura Gaussiana:** nodos y pesos Ã³ptimos

---

## ğŸŸ¢ Regla del Trapecio

### ğŸ”§ AproximaciÃ³n

Se interpola una recta entre $(a, f(a))$ y $(b, f(b))$:

$$
\int_a^b f(x)\,dx \approx \frac{b-a}{2} [f(a) + f(b)]
$$

### ğŸ’¡ Ventajas

- Simple de implementar
- Ãštil para funciones lineales o suavemente curvadas

### âš ï¸ Error

$$
E_T = -\frac{(b-a)^3}{12} f''(\xi)
$$

- Depende de la segunda derivada de $f$
- Se puede reducir dividiendo en subintervalos

## ğŸ’» Regla del Trapecio Compuesta

$$
\int_a^b f(x)\,dx \approx \frac{h}{2} \left( f(x_0) + 2\sum_{i=1}^{n-1} f(x_i) + f(x_n) \right)
$$

con $h = \frac{b-a}{n}$

$$
E_T^{(comp)} = -\frac{(b-a)h^2}{12} f''(\xi)
$$

---

## ğŸŸ¢ Regla de Simpson 1/3

### ğŸ”§ AproximaciÃ³n

Se interpola un polinomio cuadrÃ¡tico entre 3 puntos:

$$
\int_a^b f(x)\,dx \approx \frac{b-a}{6} [f(a) + 4f\left( \frac{a+b}{2} \right) + f(b)]
$$

## ğŸ’» Regla de Simpson 1/3 Compuesta

Requiere **n par** subintervalos:

$$
\int_a^b f(x)\,dx \approx \frac{h}{3} \left[ f(x_0) + 4\sum_{\text{impares}} f(x_i) + 2\sum_{\text{pares}} f(x_i) + f(x_n) \right]
$$

### âš ï¸ Error

$$
E_S = -\frac{(b-a)^5}{180} f^{(4)}(\xi)
$$

- Mucho mÃ¡s preciso que el trapecio si $f$ es suave
- Usa segunda y cuarta derivada

---

## ğŸŸ¢ Cuadratura de Gauss

### ğŸ”§ Objetivo

Aproximar:

$$
\int_{-1}^{1} f(x)\,dx \approx \sum_{i=1}^{n} w_i f(x_i)
$$

donde:

- $x_i$: raÃ­ces del polinomio de Legendre de grado $n$
- $w_i$: pesos asociados

> Puede integrarse con exactitud polinomios de grado $2n-1$ con solo $n$ puntos.

### ğŸ’¡ Ventajas

- Mucho mÃ¡s precisa que Newton-Cotes con pocos puntos
- No requiere que los nodos estÃ©n equiespaciados
- Chapra y Burden recomiendan para integrales complicadas

### ğŸ’» Cambio de intervalo

Para transformar $[a,b]$ a $[-1,1]$:

$$
\int_a^b f(x)\,dx = \frac{b-a}{2} \int_{-1}^{1} f\left( \frac{b-a}{2}x + \frac{a+b}{2} \right)\,dx
$$

### ğŸ“˜ Tablas (para Gauss-Legendre)

Para $n = 2$:

$$
x_1 = -\frac{1}{\sqrt{3}}, \quad x_2 = \frac{1}{\sqrt{3}} \quad\text{y}\quad w_1 = w_2 = 1
$$

Para $n = 4$, los nodos y pesos son fracciones con raÃ­ces cuadradas (ver tabla en apunte anterior).

---

## ğŸ“‰ Comparativa de MÃ©todos

| MÃ©todo             | Orden  | Nodos equiespaciados | PrecisiÃ³n relativa | Observaciones         |
| ------------------ | ------ | -------------------- | ------------------ | --------------------- |
| Trapecio simple    | 2      | âœ…                   | Baja               | Sencillo              |
| Trapecio compuesto | 2      | âœ…                   | Media              | Mejora con n          |
| Simpson 1/3        | 4      | âœ… (n par)           | Alta               | Muy usado             |
| Simpson 3/8        | 4      | âœ… (n mÃºltiplo de 3) | Similar a 1/3      | Poco mÃ¡s complejo     |
| Gauss-Legendre     | $2n-1$ | âŒ                   | Muy alta           | Nodos y pesos Ã³ptimos |

---

## ğŸ“Œ Observaciones finales (Chapra/Burden)

- A mayor grado del polinomio, mayor el riesgo de oscilaciones â†’ usar con cuidado
- MÃ©todos compuestos (Simpson/Trapecio) son preferibles a globales
- Cuadratura de Gauss es ideal cuando se busca **alta precisiÃ³n con pocos puntos**
- Siempre tener en cuenta el comportamiento de las derivadas al estimar el error
