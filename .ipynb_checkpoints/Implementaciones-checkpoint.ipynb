{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "969f07af-6283-4c5f-b284-147a7cebd790",
   "metadata": {},
   "source": [
    "# Unidad 3 - Sistemas de ecuaciones lineales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313e3719-6372-4074-b2bf-d28f24b0107b",
   "metadata": {},
   "source": [
    "## 🔍 Objetivo\n",
    "\n",
    "Resolver sistemas del tipo:\n",
    "\n",
    "$$\n",
    "A \\cdot x = b\n",
    "$$\n",
    "\n",
    "donde $A \\in \\mathbb{R}^{n \\times n}$, $x$ es el vector incógnita y $b$ el vector de constantes. Se usan métodos numéricos para evitar errores acumulativos y reducir el costo computacional.\n",
    "\n",
    "## 🔸 Tipos de Matrices\n",
    "\n",
    "**DENSAS:** Son aquellas que poseen pocos elementos nulos y son de orden bajo\\\n",
    "**RALAS (SPARSE):** Son aquellas que poseen muchos ceros y son de orden alto\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352b1925-5911-41c6-a06d-160162860be1",
   "metadata": {},
   "source": [
    "## 🔸 Tipos de Métodos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f35d7d9-aecf-40d8-a909-0a2a7c46fec2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1. **Métodos Directos**\n",
    "Son aquellos que nos conducirían a la solución exacta luego de un número finito de operaciones elementales, si no hubiera errores de redondeo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adc575b-8fa4-4dc3-b148-85859eb93755",
   "metadata": {},
   "source": [
    "### 🟢  **Eliminación de Gauss (o Reducción Gaussiana) $O(n^3)$**\n",
    "\n",
    "* Transforma el sistema en forma escalonada superior usando operaciones elementales a la matriz ampliada (modifica b).\n",
    "* Se resuelve con **sustitución hacia atrás**.\n",
    "* Puede incluir **pivoteo parcial** para mejorar estabilidad numérica.\n",
    "\n",
    "**Errores comunes:**\n",
    "\n",
    "* El error de redondeo se acumula en cada paso, especialmente sin pivoteo.\n",
    "* Matrices mal condicionadas pueden amplificar errores.\n",
    "\n",
    "\n",
    "### 💡 Ventajas\n",
    "\n",
    "* Método **general**: se puede aplicar a cualquier sistema no singular.\n",
    "* Base de otros métodos como LU, Thomas (caso tridiagonal), y muchos algoritmos numéricos.\n",
    "* Se puede mejorar con:\n",
    "\n",
    "  * **Pivoteo parcial** (mayor estabilidad).\n",
    "  * **Escalamiento** (evita errores por magnitudes muy diferentes).\n",
    "### ⚠️ Requisitos\n",
    "\n",
    "* La matriz debe ser **no singular**.\n",
    "* Sin pivoteo, puede fallar o generar grandes errores numéricos si hay ceros o valores muy pequeños en la diagonal.\n",
    "### 💻 Complejidad\n",
    "\n",
    "* Eliminación (reducción a triangular superior):$  O\\left( \\frac{2}{3} n^3 \\right)$\n",
    "* Sustitución hacia atrás:$  O(n^2)  $\n",
    "* **Total:** $O(n^3)$\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc021996-8585-4aa7-b908-638cf27b948d",
   "metadata": {},
   "source": [
    "\n",
    "### 🟢  **Factorización LU** $O(n^3)$\n",
    "\n",
    "#### 🔍 Objetivo\n",
    "Descompone $A = L \\cdot U$, donde $A$ debe ser **no singular**, y para LU sin pivoteo, debe ser posible evitar ceros en la diagonal de $U$:\n",
    "\n",
    "  * $L$: matriz triangular inferior con unos en la diagonal\n",
    "  * $U$: matriz triangular superior\n",
    "* Luego resuelve $L \\cdot y = b$, y $U \\cdot x = y$\n",
    "\n",
    "#### 💡 Ventajas\n",
    "* Útil para resolver muchos sistemas con la misma matriz $A$ y distintos vectores $b$.\n",
    "* Se puede calcular $|A| = |L.U| = |U|$\n",
    "* Cálculo de $A^{-1}$ usando como b las columnas canónicas\n",
    "* Se puede combinar con **pivoteo parcial** para mayor estabilidad:\n",
    "\n",
    "### 💻 Complejidad\n",
    "* **Factorización LU:** $ O\\left( \\frac{2}{3} n^3 \\right)  $\n",
    "* **Sustituciones (hacia adelante y hacia atrás):**  $  O(n^2)  $\n",
    "* **Total por un sistema:**  $  O(n^3)  $\n",
    "* **Total si ya tenés la factorización (solo resolver para otro $b$):**  $  O(n^2)  $\n",
    "---\n",
    "\n",
    "### **Pivoteo parcial**\n",
    "\n",
    "* Consiste en, en cada paso de la eliminación de Gauss, buscar en la columna actual el elemento con mayor valor absoluto con el objetivo de evitar divisiones por números muy pequeños (o $0$) que aumentan errores de redondeo. **entre las filas restantes** (desde la fila pivote hacia abajo) e inicializar un vector $P$. \n",
    "\n",
    "* **Vector $P$ (permutaciones):**\n",
    "  Registra el orden actual de las filas luego de los intercambios durante la eliminación. Inicialmente, $P = [1, 2, ..., n]$. Cada vez que se intercambian filas $i$ y $k$, se intercambian también $P_i \\leftrightarrow P_k$. Útil para mantener la correspondencia entre filas originales y filas actuales, sin mover físicamente toda la matriz (optimización).\n",
    "\n",
    "\n",
    "###  **Escalamiento Implicito**\n",
    "\n",
    "* Wilkinson propone que una matriz debe equilibrarse antes de aplicar una algoritmo de solucion. Se basa en **normalizar la comparación de valores en cada fila según la magnitud relativa en su fila** para elegir mejor el pivote. Por lo que para cada fila $i$, se calcula su factor de escala en $S$:\n",
    "\n",
    "* **Vector $S$ (escalas):**\n",
    "  Guarda para cada fila $i$ el valor máximo absoluto de sus coeficientes en la matriz original $A$:\n",
    "\n",
    "$$\n",
    "S_i = \\max_{1 \\leq j \\leq n} |a_{ij}|\n",
    "$$\n",
    "\n",
    "\n",
    "* Al elegir el pivote, no se mira solo el valor absoluto del elemento de la columna $k$, sino el cociente: $ c_i = \\frac{|a_{ik}|}{S_i} $ haciendo una comparación justa y relativa\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "### ⚙️ Funcionamiento resumido del ppe:\n",
    "\n",
    "1. Se calcula $S$ antes de comenzar la eliminación y $P$ se inicializa desde 1 hasya n.\n",
    "2. En cada paso $k$, para elegir fila pivote $p$, se evalúa: $ c_i = \\frac{|a_{ik}|}{S_i}$\n",
    "\n",
    "3. Se intercambian filas $k$ y $p$ tanto en la matriz como en $P$.\n",
    "4. Se continúa con la eliminación habitual.\n",
    "5. Al resolver queda: $PLU x = b \\implies L y = P b \\implies U x = y$. Es decir, debemos aplicar la permutacion a b.\n",
    "\n",
    "\n",
    "#### 📝 Beneficios\n",
    "\n",
    "* Permite hacer pivoteo con una comparación justa considerando la escala de cada fila.\n",
    "* Mejora estabilidad y evita errores numéricos.\n",
    "* El vector $P$ facilita reconstruir la solución o aplicar permutaciones posteriores sin perder datos.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101c910e-0fa6-4070-972e-0ce78ffea0ca",
   "metadata": {},
   "source": [
    "### 🟢 Descomposición de Cholesky\n",
    "\n",
    "#### 🔍 Objetivo\n",
    "\n",
    "Resolver $A x = b$ cuando $A$ es **simétrica y definida positiva**.\n",
    "\n",
    "#### 📐 Descomposición:\n",
    "\n",
    "$$\n",
    "A = L \\cdot L^T\n",
    "$$\n",
    "\n",
    "* $L$: matriz triangular inferior con coeficientes reales, incluso en su diagonal.\n",
    "* Luego se resuelven:\n",
    "\n",
    "$$\n",
    "L y = b \\quad \\text{(sustitución hacia adelante)}  \n",
    "$$\n",
    "\n",
    "$$\n",
    "L^T x = y \\quad \\text{(sustitución hacia atrás)}\n",
    "$$\n",
    "\n",
    "### 🧮 Fórmulas por componentes\n",
    "\n",
    "Para construir $L$, se recorren filas y columnas de forma incremental. Los elementos se calculan así:\n",
    "\n",
    "#### 🟩 Diagonal ($i = j$):\n",
    "\n",
    "$$\n",
    "\\ell_{ii} = \\sqrt{ a_{ii} - \\sum_{k=1}^{i-1} \\ell_{ik}^2 }\n",
    "$$\n",
    "#### 🟦 Debajo de la diagonal ($i > j$):\n",
    "\n",
    "$$\n",
    "\\ell_{ij} = \\frac{1}{\\ell_{jj}} \\left( a_{ij} - \\sum_{k=1}^{j-1} \\ell_{ik} \\ell_{jk} \\right)\n",
    "$$\n",
    "\n",
    "### 💡 Ventajas\n",
    "\n",
    "* Es **más eficiente** y **más estable** que la LU tradicional si se cumplen las condiciones.\n",
    "* Requiere casi la **mitad del trabajo** que LU.\n",
    "* Ideal para problemas de ingeniería con matrices simétricas dispersas.\n",
    "\n",
    "### 💻 Complejidad\n",
    "\n",
    "* Descomposición: $O\\left( \\frac{1}{3} n^3 \\right)$ (mitad que LU)\n",
    "* Sustituciones: $O(n^2)$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0482c442-ff60-4887-8b88-9bd2873ab1e4",
   "metadata": {},
   "source": [
    "\n",
    "### 🟢 Método de Thomas (tridiagonal)\n",
    "\n",
    "#### 🔍 Objetivo\n",
    "\n",
    "Resolver sistemas donde $A$ es **tridiagonal**, sólo tiene elementos distintos de cero en la diagonal principal y las diagonales adyacentes, es decir:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}f_1 & g_1 & 0   & \\cdots & 0 \\\\e_2 & f_2 & g_2 & \\cdots & 0 \\\\0   & e_3 & f_3 & \\ddots & \\vdots \\\\\\vdots & \\ddots & \\ddots & \\ddots & g_{n-1} \\\\0 & \\cdots & 0 & e_n & f_n\\end{bmatrix} .\n",
    "\\begin{bmatrix}x_1 \\\\x_2 \\\\x_3 \\\\\\vdots \\\\x_{n-1} \\\\x_n\\end{bmatrix} =\n",
    "\\begin{bmatrix}r_1 \\\\r_2 \\\\r_3 \\\\\\vdots \\\\r_{n-1} \\\\r_n\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### 🧠 Algoritmo\n",
    "#### Descomposición\n",
    "Para $k = 2$ hasta $n$: $e_k = \\frac{e_k}{f_{k-1}}$; $f_k = f_k - e_k \\cdot g_{k-1} $\n",
    "\n",
    "#### Sustitución hacia adelante\n",
    "Para $k = 2$ hasta $n$: $r_k = r_k - e_k \\cdot r_{k-1} $    \n",
    "#### Sustitución hacia atrás\n",
    "$x_n = \\frac{r_n}{f_n}$ \\\n",
    "Para $k = n-1$ hasta $1, -1$ : $x_k = \\frac{r_k - g_k \\cdot x_{k+1}}{f_k} $\n",
    "     \n",
    "      \n",
    "Es una forma optimizada de **eliminación de Gauss** que aprovecha la estructura tridiagonal para:\n",
    "\n",
    "1. **Eliminar los elementos debajo de la diagonal** de forma eficiente\n",
    "2. **Resolver con sustitución hacia atrás**\n",
    "\n",
    "### 💡 Ventajas\n",
    "\n",
    "* No requiere almacenar toda la matriz → solo 3 vectores\n",
    "* Precisión superior a métodos genéricos al reducir operaciones\n",
    "\n",
    "\n",
    "### 💻 Complejidad\n",
    "\n",
    "* Fase de eliminación: $O(n)$\n",
    "* Fase de sustitución: $O(n)$\n",
    "* **Total:** $O(n)$\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351ce7be-ba14-4293-833d-d2dcbdff4059",
   "metadata": {},
   "source": [
    "\n",
    "### 📌 Comparativa final de métodos directos\n",
    "\n",
    "\n",
    "| Método       | Requisitos principales                 | Complejidad                                    | Ventajas clave                                                           |\n",
    "| ------------ | -------------------------------------- | ---------------------------------------------- | ------------------------------------------------------------------------ |\n",
    "| **Gauss**    | No singular y $a_{ii} \\neq 0$ (sin pivoteo)                    | $O(n^3)$                                       | General, robusto, base de muchos otros métodos                           |\n",
    "| **LU**       | No singular y $a_{ii} \\neq 0$ (sin pivoteo)                    | $O(n^3)$ (una vez) <br> $O(n^2)$ (por sistema) | Reutilizable para varios $b$, más eficiente que Gauss en ese caso        |\n",
    "| **Cholesky** | **Simétrica definida positiva** | $O\\left(\\frac{1}{3} n^3 \\right)$               | Más rápido y estable que LU si aplica; usa menos operaciones             |\n",
    "| **Thomas**   | **Tridiagonal**                 | $O(n)$                                         | Extremadamente eficiente; ideal para sistemas grandes con esa estructura |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e321ee-438b-48e9-bc39-d4fb480fb044",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2. **Métodos Iterativos**\n",
    "\n",
    "Parten de una estimación inicial $x_0$ y construyen una sucesión de aproximaciones, que en principio, convergen a la solución x. Más eficientes en matrices grandes y dispersas.\n",
    "\n",
    "#### a. **Método de Jacobi**\n",
    "\n",
    "Iteración:\n",
    "\n",
    "$$\n",
    "x_i^{(k+1)} = \\frac{1}{a_{ii}} \\left( b_i - \\sum_{j \\ne i} a_{ij} x_j^{(k)} \\right)\n",
    "$$\n",
    "\n",
    "**Condiciones para convergencia:**\n",
    "\n",
    "* $A$ debe ser **diagonalmente dominante** o **simétrica positiva definida**.\n",
    "\n",
    "#### b. **Método de Gauss-Seidel**\n",
    "\n",
    "Iteración (usa valores actualizados a medida que se calculan):\n",
    "\n",
    "$$\n",
    "x_i^{(k+1)} = \\frac{1}{a_{ii}} \\left( b_i - \\sum_{j < i} a_{ij} x_j^{(k+1)} - \\sum_{j > i} a_{ij} x_j^{(k)} \\right)\n",
    "$$\n",
    "\n",
    "**Convergencia:** más rápida que Jacobi en general, mismas condiciones de convergencia.\n",
    "\n",
    "#### c. **Método de Relajación (SOR)**\n",
    "\n",
    "Introduce un factor de relajación $\\omega$:\n",
    "\n",
    "$$\n",
    "x_i^{(k+1)} = (1 - \\omega)x_i^{(k)} + \\frac{\\omega}{a_{ii}} \\left( b_i - \\sum_{j < i} a_{ij} x_j^{(k+1)} - \\sum_{j > i} a_{ij} x_j^{(k)} \\right)\n",
    "$$\n",
    "\n",
    "* $\\omega = 1$: método de Gauss-Seidel\n",
    "* $0 < \\omega < 2$: ajusta la velocidad de convergencia\n",
    "\n",
    "---\n",
    "\n",
    "## 📏 Tipos de Errores\n",
    "\n",
    "### 🔹 Error de Redondeo\n",
    "\n",
    "* Se acumula en cada operación aritmética.\n",
    "* Especialmente relevante en métodos directos sin pivotamiento.\n",
    "\n",
    "### 🔹 Error de Truncamiento\n",
    "\n",
    "* Poco significativo en métodos directos, pero **clave en iterativos**: cortar la iteración antes de la solución exacta.\n",
    "\n",
    "---\n",
    "\n",
    "## 📐 Condición del Sistema\n",
    "\n",
    "### ✅ Matriz Bien Condicionada:\n",
    "\n",
    "* Pequeñas perturbaciones en $A$ o $b$ → pequeños cambios en $x$\n",
    "\n",
    "### ❌ Matriz Mal Condicionada:\n",
    "\n",
    "* Pequeñas perturbaciones en los datos → grandes errores en la solución\n",
    "\n",
    "Se mide con el **número de condición** $\\kappa(A) = \\|A\\| \\cdot \\|A^{-1}\\|$\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Fórmulas Clave\n",
    "\n",
    "* **Error relativo en iterativos**:\n",
    "\n",
    "  $$\n",
    "  \\varepsilon^{(k)} = \\frac{\\|x^{(k+1)} - x^{(k)}\\|}{\\|x^{(k+1)}\\|}\n",
    "  $$\n",
    "\n",
    "* **Número de condición (aproximado):**\n",
    "\n",
    "  $$\n",
    "  \\kappa(A) \\approx \\frac{\\text{mayor valor singular}}{\\text{menor valor singular}}\n",
    "  $$\n",
    "\n",
    "* **Diagonal dominante (suficiente para convergencia):**\n",
    "\n",
    "  $$\n",
    "  |a_{ii}| > \\sum_{j \\ne i} |a_{ij}|\n",
    "  $$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
